<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Syntactic Tree Kernels | Matt's Log</title><meta name=keywords content><meta name=description content="The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge)."><meta name=author content="Matt Goodman"><link rel=canonical href=https://goodmattg.github.io/posts/syntactic-tree-kernels/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://goodmattg.github.io/magnet.ico><link rel=icon type=image/png sizes=16x16 href=https://goodmattg.github.io/magnet-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://goodmattg.github.io/magnet-32x32.ico><link rel=apple-touch-icon href=https://goodmattg.github.io/magnet-apple-touch-icon.png><link rel=mask-icon href=https://goodmattg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Syntactic Tree Kernels"><meta property="og:description" content="The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge)."><meta property="og:type" content="article"><meta property="og:url" content="https://goodmattg.github.io/posts/syntactic-tree-kernels/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-04T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-04T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Syntactic Tree Kernels"><meta name=twitter:description content="The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://goodmattg.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Syntactic Tree Kernels","item":"https://goodmattg.github.io/posts/syntactic-tree-kernels/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Syntactic Tree Kernels","name":"Syntactic Tree Kernels","description":"The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge).","keywords":[],"articleBody":" Motivation This post encapsulates the work I did with Dean Fulgoni for our final project in Big Data Analytics. Credit to Professor Chris Callison-Burch for steering me away from some topics that “could easily be Phd theses” and to Professor Zachary Ives for being an excellent teacher.\nThe Problem We Tried to Solve The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge). Not to get off track, but this begs some thought on the nature of what a question is and whether a question is separable from its posed context. See the [Stanford Encyclopedia of Philosophy (SEP)]1 for an excellent review of the philosophy of questions. I won’t be discussing it in detail, but my view on this problem is colored by the philosophical understanding of questions. We can assume that questions on Quora are requesting information, not testing the knowledge of the user base.\n… a question [is] an abstract thing for which an interrogative sentence is a piece of notation. (Belnap and Steel, 1976)\nThe challenge was posed as a supervised learning question with a human labeled training set. A cursory glance at the training set showed that the human labeling was inconsistent, with varying standards to classify questions as semantically the same.\nThese were labeled as asking two different questions:\nDo we need smart cities or smart politicians? Do we need smart cities or smart laws? We can see that the classification of this pair is debatable. Politicians are not laws. However, politicians are the only people with the power to make laws. Where is the line drawn for semantic similarity? How do we even approach situations where the the map between language and abstract meaning is complex?\n$$ \\begin{aligned} Politician \u0026\\ne Law \\cr \\\\ Politician \u0026\\rightarrow Law \\cr \\end{aligned} $$\nWith an unclear standard for determining whether two questions had the same intent, we decided to approach the problem ’loosely’. Overfitting in any specific approach would kill our performance on the test set. We partitioned the feature set into syntactic features and semantic features. I’ll touch on semantic features later, but I decided to focus on syntactic features because it was much lower hanging fruit. The question became how to quantify the syntactic similarity of two sentences.\n$$ f: (sentence_1, sentence_2) \\rightarrow s \\in [0,1] $$\nI’ll save the complete literature review for a separate post. We chose to use Tree Kernels championed by Prof. Alessandro Moschitti of the University of Trento, Italy because of the elegance of the theoretical basis.\nSyntactic Tree Kernel Theory As part of our search for unique features to encode syntactic and semantic similarity, we found that so called Tree Kernels had been shown to be highly effective in augmenting SVMs designed to solve problems in question classification, question answering, Semantic Role Labeling, and named entity recognition. The broad idea behind the theory is that syntactic features must be quantified to learn semantic structures. It isn’t enough to say, compare lists of POS tags for two sentences because the structure of the sentences shape their meanings. We therefore design kernels that map from tree structures (e.g. constituency parse trees) to scores that measure the syntactic similarity of the two trees. Note that these scores are bounded for use in an SVM framework, but could easily be used as features in a deep learning framework.\nKernel Definition The next few section are taken almost verbatim from Moschitti’s ACL 2012 tutorial on State-of-the-art Kernels in Natural Language Processing. All credit goes to him and his colleagues.\nThe point of the following math is that we can ensure our SVM kernel is valid only if the matrix of transformed points is positive semi-definite. You can review the literature for the proof.\nA kernel is a function:\n$$ k: (\\overrightarrow{x},\\overrightarrow{z}) \\rightarrow \\Phi(\\overrightarrow{x}) \\cdot \\Phi(\\overrightarrow{z}) ~~~~~~\\forall \\overrightarrow{x}, \\overrightarrow{z} \\in X$$\nMercer’s Conditions for a valid kernel:\nLet $X$ be a finite input space and let $K(x,z)$ be a symmetric function on $X$. Then $K(x,z)$ is a kernel function IFF matrix:\n$$ k(x,z) = \\Phi(x) \\cdot \\Phi(z)$$\nis positive semi-definite (non-negative eigenvalues). That is, if the matrix is positive semi-definite we can find a $\\Phi$ that implements the kernel function $k$.\nTree Definition We worked exclusively with constituency parse trees. Anywhere I write “tree”, I am referring to a constituency parse tree. A constituency parse tree breaks down a sentence into phrases using a well-defined, context-free, phrase structure grammar. A context-free grammar is a set of rules that describes all possible strings in a formal language. Given any sentence, we have a formal way to define that sentence as made up of phrases, nouns, verbs, etc. Noam Chomsky was the first define context-free grammars in linguistics to describe the universal structure of natural language. We use the Penn-Treebank II formal grammar for this project. Here is a small sample of the Penn-Treebank II formal grammar:\nRB: Adverb IN: Preposition PP: Prepositional Phrase VP: Verb Phrase Below is an example of a constituency parse tree for the sentence:\nI will never stop learning.\nNotice that sentence tokens (the words) are always leaves of the tree and in their natural language order. That is, we should always be able to read the tree leaves from left to right and have our exact sentence.\nTree Kernel Core Theory and Definitions A tree kernel is a function: $f: (tree_1, tree_2) \\rightarrow \\mathbf{R}^+ $\nA tree is defined recursively as a set of nodes, each of which has a value and a set of children, each of which are trees. A node is a “leaf” if it has no children. A node is “pre-terminal” if all of its child nodes are leaves. The “production” of a node is the subtree containing the node and its children. The production of two nodes are the same if the nodes have the same value and their children have the same value. Because we work are working with ordered trees, the children must have the same values in the same order to be part of the same production.\nKey to the effectiveness of tree kernel features in solving problems is the choice of tree structure. Moschitti shows that different tree kernels are effective for different choices of syntax trees in solving different classes of problems. We used Stanford NLP to generate parse trees from the input sentences. A parse tree is an ordered tree that represents the syntax of a sentence according to some context-free grammar. There are two main classes of parse trees: constituency and dependency. A dependency parse trees is the tree structure generated by the dependencies of a sentence. While dependencies can be highly useful in generating features, as described in Section 4.1, Moschitti found that constituency parse trees were most effective in solving classification related problems. A constituency parse tree breaks down the tokens of sentence into their constituent grammars.\nSyntactic Tree Kernel or “Collins-Duffy” (STK) For our tree kernel features we only implemented the Collins-Duffy tree kernel. The general idea of the Collins-Duffy Tree Kernel is that we want to compare the number of common subtrees between two input trees. The naive approach in exponential time is to enumerate all possible subtrees of the two input trees and then perform the dot-product. Obviously generating every subtree of each sentence, and then taking the dot-product of all of the subtrees to find how many the two sentences have in common would be a computationally nightmarish $O(n^2)$. We can take advantage of the fact that generating subtrees is recursive, and that if two parent nodes do not match, the subtrees rooted at those two nodes do not match. Collins and Duffy propose the following derivation.\nDefine the tree kernel $K: f(T_1,T_2) \\rightarrow \\mathbf{R}^+$ where $T_1$ and $T_2$ are well-formed trees. The dot-product approach above is written:\n$$K(T_1,T_2) = h(T_1) \\cdot h(T_2)$$\nWhere $h$ is a function that produces all of the subtrees of a tree. Again, the dot-product between $h(T_1)$ and $h(T_2)$ will count all of the subtrees $T_1$ and $T_2$ have in common. They define the indicator function $I_i(n)$ to be 1 if the subtree $i$ is rooted at node $n$ and 0 otherwise. Therefore, we can rewrite the dot-product above.\n$$ \\begin{aligned} h(T_1) \\cdot h(T_2) \u0026= \\sum_i h_i(T_1)h_i(T_2) \\\\ \u0026 = \\sum_{n_1 \\in N_1}\\sum_{n_2 \\in N_2} \\sum_i I_i(n_1) I_i(n_2) \\\\ \u0026 = \\sum_{n_1 \\in N_1}\\sum_{n_2 \\in N_2} C(n_1, n_2) \\end{aligned} $$\nWhere $C(n_1,n_2)$ is defined as $\\sum_i I_i(n_1)I_i(n_2)$. We note that $C(n_1, n_2)$ can be computed recursively in polynomial time:\n$C(n_1,n_2) = 0$ if the productions at $n_1$ and $n_2$ are different.\n$C(n_1,n_2) = \\lambda$ if the productions at $n_1$ and $n_2$ are the same and $n_1$ and $n_2$ are pre-terminal.\n$$C(n_1,n_2) = \\lambda \\prod_{j=1}^{nc(n_1)}(\\sigma + C(ch(n_1,j),ch(n_2,j)))$$\nWhere $0 \u003c \\lambda \\leq 1$ is a factor that down weights subtrees exponentially with their size, $nc(n_i)$ returns the number of children of node $i$, binary variable $\\sigma \\in {0,1}$ determines whether we use the ST or SST kernel, and $ch(n_i,j)$ selects the $j$th child of node $i$. The ST or subtree kernel is where $\\sigma = 0$. For the ST kernel, the entire subtree must match to return a non-zero score. The SST or subset tree kernel is where $\\sigma = 1$. For the SST kernel, non-zero scores can be achieved even if all of the subtrees of two nodes do not match.\nThe distinction between the ST and SST kernels in the previous section is that the SST kernel is more forgiving than the ST kernel. The SST kernel can still return high scores even if the exact sentences are not the same. Likewise, the ST kernel heavily penalizes minor differences between constituency trees. The factor $\\lambda$ can be viewed as a penalty we utilize so that kernel does not over-inflate the importance of matches between larger subtrees. These parameters become less important when we normalize the kernel score using:\n$$K_{norm}(T_1,T_2) = \\frac{K(T_1,T_2)}{K(T_1,T_1)K(T_2,T_2)}\nWe generated features with the ST and SST tree kernels for each question pair using $\\lambda \\in {1, 0.8, 0.5, 0.2, 0.1, 0.05, 0.2}$ totaling 14 tree kernel features. Determining the optimal $\\lambda$ for this task would be a good exploration on its own.\nSynactic Tree Kernels in Python As far as I know, this is the first implementation of syntactic tree kernels in Python. This code may eventually move to its own maintained repository if it’s needed. If you find any bugs please reach out to me. Dr. Moschitti’s cited implementation is:\n[SVM-Light]2 written by Thorsten Joachims in C. His implementation is most-likely faster (no comparison benchmark yet). My implementation is however easier to grasp and is isolated from a complete SVM library.\nBuilding the Parse Tree As described in the Pipeline section we used Stanford CoreNLP to generate our constituency parse trees. There aren’t many libraries that are good for constituency parsing, and CoreNLP is by far the most stable and customizable. Oddly enough, Stanford CoreNLP will print a constituency parse tree for a given sentence with well-defined structure to console (i.e. pretty print), but provides no functionality to store the tree in a reasonable format (linked list, hashmap, etc. ). The code in this subsection is available on my [Github]3 and is used to take the raw text output of Stanford CoreNLP and store in the schema below for tree kernel computation. We label each token with an Id and store the tokens for fast computation, but admittedly poor space complexity.\ncurid: Integer parid: Integer posOrTok: String indent: Integer children: [Integer] childrenTok: [String] Computing Tree Kernels 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 import numpy as np import math from nltk.corpus import wordnet as wn ''' Helper Methods ''' def _isLeaf_(tree, parentNode): return (len(tree[parentNode]['children']) == 0) def _isPreterminal_(tree, parentNode): for idx in tree[parentNode]['children']: if not _isLeaf_(tree, idx): return False return True ''' Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel ''' def _cdHelper_(tree1, tree2, node1, node2, store, lam, SST_ON): # No duplicate computations if store[node1, node2] \u003e= 0: return # Leaves yield similarity score by definition if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)): store[node1, node2] = 0 return # same parent node if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']: # same children tokens if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']: # Check if both nodes are pre-terminal if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2): store[node1, node2] = lam return # Not pre-terminal. Recurse among the children of both token trees. else: nChildren = len(tree1[node1]['children']) runningTotal = None for idx in range(nChildren): # index -\u003e node_id tmp_n1 = tree1[node1]['children'][idx] tmp_n2 = tree2[node2]['children'][idx] # Recursively run helper _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON) # Set the initial value for the layer. Else multiplicative product. if (runningTotal == None): runningTotal = SST_ON + store[tmp_n1, tmp_n2] else: runningTotal *= (SST_ON + store[tmp_n1, tmp_n2]) store[node1, node2] = lam * runningTotal return else: store[node1, node2] = 0 else: # parent nodes are different store[node1, node2] = 0 return def _cdKernel_(tree1, tree2, lam, SST_ON): # Fill the initial state of the store store = np.empty((len(tree1), len(tree2))) store.fill(-1) # O(N^2) to compute the tree dot product for i in range(len(tree1)): for j in range(len(tree2)): _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON) return store.sum() ''' Returns a tuple w/ format: (raw, normalized) If NORMALIZE_FLAG set to False, tuple[1] = -1 ''' def _CollinsDuffy_(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON): raw_score = _cdKernel_(tree1, tree2, lam, SST_ON) if (NORMALIZE_FLAG): t1_score = _cdKernel_(tree1, tree1, lam, SST_ON) t2_score = _cdKernel_(tree2, tree2, lam, SST_ON) return (raw_score,(raw_score / math.sqrt(t1_score * t2_score))) else: return (raw_score,-1) ''' Implementation of the Partial Tree Kernel (PTK) from: \"Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees\" by Alessandro Moschitti ''' ''' The delta function is stolen from the Collins-Duffy kernel ''' def _deltaP_(tree1, tree2, seq1, seq2, store, lam, mu, p): # # Enumerate subsequences of length p+1 for each child set if p == 0: return 0 else: # generate delta(a,b) _delta_(tree1, tree2, seq1[-1], seq2[-1], store, lam, mu) if store[seq1[-1], seq2[-1]] == 0: return 0 else: runningTot = 0 for i in range(p-1, len(seq1)-1): for r in range(p-1, len(seq2)-1): scaleFactor = pow(lam, len(seq1[:-1])-i+len(seq2[:-1])-r) dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-1) runningTot += (scaleFactor * dp) return runningTot def _delta_(tree1, tree2, node1, node2, store, lam, mu): # print(\"Evaluating Delta: (%d,%d)\" % (node1, node2)) # No duplicate computations if store[node1, node2] \u003e= 0: return # Leaves yield similarity score by definition if (_isLeaf_(tree1, node1) or _isLeaf_(tree2, node2)): store[node1, node2] = 0 return # same parent node if tree1[node1]['posOrTok'] == tree2[node2]['posOrTok']: if _isPreterminal_(tree1, node1) and _isPreterminal_(tree2, node2): if tree1[node1]['childrenTok'] == tree2[node2]['childrenTok']: store[node1, node2] = lam else: store[node1, node2] = 0 return else: # establishes p_max childmin = min(len(tree1[node1]['children']), len(tree2[node2]['children'])) deltaTot = 0 for p in range(1,childmin+1): # compute delta_p deltaTot += _deltaP_(tree1, tree2, tree1[node1]['children'], tree2[node2]['children'], store, lam, mu, p) store[node1, node2] = mu * (pow(lam,2) + deltaTot) return else: # parent nodes are different store[node1, node2] = 0 return def _ptKernel_(tree1, tree2, lam, mu): # Fill the initial state of the store store = np.empty((len(tree1), len(tree2))) store.fill(-1) # O(N^2) to compute the tree dot product for i in range(len(tree1)): for j in range(len(tree2)): _delta_(tree1, tree2, i, j, store, lam, mu) return store.sum() ''' Returns a tuple w/ format: (raw, normalized) If NORMALIZE_FLAG set to False, tuple[1] = -1 ''' def _MoschittiPT_(tree1, tree2, lam, mu, NORMALIZE_FLAG): raw_score = _ptKernel_(tree1, tree2, lam, mu) if (NORMALIZE_FLAG): t1_score = _ptKernel_(tree1, tree1, lam, mu) t2_score = _ptKernel_(tree2, tree2, lam, mu) return (raw_score,(raw_core / math.sqrt(t1_score * t2_score))) else: return (raw_score,-1) Towards Syntactic-Semantic Tree Kernels Ignoring all of the theory and code in the previous sections would not be unreasonable. The major criticism of ST and PTK kernels is that they are syntactic, and syntax is not meaning. A simple example:\nThe girl went to the store to buy eggs for the family. The girl went to the store to buy milk for the family. The sentences generate the same constituency parse tree (minus) the differing terminal leaf, but have completely different meanings. Put plainly, the sentences above are grammatically the same except for the one word difference. However, the SST kernel we implemented above would give a very high similarity score for the meaning of the two sentences even though we know that they have different meanings. Bloehdorn and Moschitti propose a theoretically elegant modification to solve our problem. This new syntactic-semantic tree kernel (SSTK) is defined as:\n$$ K_{SSTK} = comp(f_1, f_2) \\prod_{t=1}^{nt_{(f1)}} SIM(f_1(t),f_2(t)) $$\nwhere $comp(f_1, f_2)$ is $1$ if $f_1$ differs from $f_2$ only in the terminal nodes and 0 otherwise, $nt(f_i)$ is the number of terminal nodes and $f_i(t)$ is the t-th terminal symbol of $f_i$ numbered from left to right, and $SIM$ is a function returning $[0, 1]$ where $0$ is completely different, and $1$ means the words are identical in meaning.\nNow in plain English. If the two tree fragments don’t have the same non-terminal structure then we end the computation early. The SSTK is the same as PTK and SST in that it ignores tree fragments that are not identical in syntactic structure (grammar). Say the two tree fragments have the same non-terminal structure, but different terminal nodes (leaves). This means the two sentence structures have the same syntax but different words. So our indicator function $comp(f_1, f_2)$ returns 1, and we compute the multiplicative product by computing the similarity of each in-order pair of terminal leaves.\nThis is where it gets admittedly ridiculous. How exactly do we quantify the semantic similarity of two words. Doesn’t this depend on the sentence context? This is where there is no good answer yet. There are computational kernels that give similarity metrics on word databases like WordNet, but we don’t have significant evidence on which is best for different classes of problems. Our implementation used the raw score returned by WordNet, but that isn’t very interesting.\nLesson Learned, Hardly Earned Research tools like StanfordCoreNLP are not performant. Most my time was spent building tooling, testing my implementations of paper algorithms, running batch jobs on AWS instances, debugging the batch processing jobs on AWS, and questioning my sanity for embarking on this rabbit hole.\nIf We Only Had More Time I would have worked on:\n[1] Reworking this kernel as a feature generator in a deep learning approach. Consider that the behavior of this entire field is based around using these as a mathematically valid kernel in an SVM. Deep learning has showed greater promise in solving these types of problems because the neural network structure is more adaptable.\n[2] Grammars: A Google Trends search shows \u003e85% Quora’s user base can be divided as living on the Indian subcontinent and the United States, yet English is almost exclusively the only language used to ask/answer questions. This matters because we should not expect perfect English grammar from people whose first language isn’t English. Any effective model should detect common misuses of English grammar specific to users from the Indian subcontinent and subsequently “loosen”. Note that this is different from detecting common colloquial expressions or slang that native English speakers use.\n[3] More work on incorporating different word similarity databases into the SSTK kernel. This could be anything from Wordnet, to scraped data from forums, to the paraphrase database (PPDB) at UPenn.\n[4] Using different metrics for WordNet semantic similarities like Resnik, Wu \u0026 Palmer, or Lin.\nLinks https://plato.stanford.edu/entries/questions/. ↩︎\nhttp://svmlight.joachims.org/ ↩︎\nhttps://github.com/goodmattg/quora_kaggle/blob/master/TreeBuild.py ↩︎\n","wordCount":"3388","inLanguage":"en","datePublished":"2017-09-04T00:00:00Z","dateModified":"2017-09-04T00:00:00Z","author":{"@type":"Person","name":"Matt Goodman"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://goodmattg.github.io/posts/syntactic-tree-kernels/"},"publisher":{"@type":"Organization","name":"Matt's Log","logo":{"@type":"ImageObject","url":"https://goodmattg.github.io/magnet.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://goodmattg.github.io accesskey=h title="Matt's Log (Alt + H)">Matt's Log</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://goodmattg.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://goodmattg.github.io/about title=About><span>About</span></a></li><li><a href=https://goodmattg.github.io/bookshelf title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://goodmattg.github.io/quotes title=Quotes><span>Quotes</span></a></li><li><a href=https://goodmattg.github.io/contact title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Syntactic Tree Kernels</h1><div class=post-description>The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge).</div><div class=post-meta><span title='2017-09-04 00:00:00 +0000 UTC'>September 4, 2017</span>&nbsp;·&nbsp;Matt Goodman</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#the-problem-we-tried-to-solve aria-label="The Problem We Tried to Solve">The Problem We Tried to Solve</a></li><li><a href=#syntactic-tree-kernel-theory aria-label="Syntactic Tree Kernel Theory">Syntactic Tree Kernel Theory</a><ul><li><a href=#kernel-definition aria-label="Kernel Definition">Kernel Definition</a></li></ul></li><li><a href=#tree-definition aria-label="Tree Definition">Tree Definition</a><ul><li><a href=#tree-kernel-core-theory-and-definitions aria-label="Tree Kernel Core Theory and Definitions">Tree Kernel Core Theory and Definitions</a></li><li><a href=#syntactic-tree-kernel-or-collins-duffy-stk aria-label="Syntactic Tree Kernel or &amp;ldquo;Collins-Duffy&amp;rdquo; (STK)">Syntactic Tree Kernel or &ldquo;Collins-Duffy&rdquo; (STK)</a></li></ul></li><li><a href=#synactic-tree-kernels-in-python aria-label="Synactic Tree Kernels in Python">Synactic Tree Kernels in Python</a><ul><li><a href=#building-the-parse-tree aria-label="Building the Parse Tree">Building the Parse Tree</a></li><li><a href=#computing-tree-kernels aria-label="Computing Tree Kernels">Computing Tree Kernels</a></li></ul></li><li><a href=#towards-syntactic-semantic-tree-kernels aria-label="Towards Syntactic-Semantic Tree Kernels">Towards Syntactic-Semantic Tree Kernels</a></li><li><a href=#lesson-learned-hardly-earned aria-label="Lesson Learned, Hardly Earned">Lesson Learned, Hardly Earned</a></li><li><a href=#if-we-only-had-more-time aria-label="If We Only Had More Time">If We Only Had More Time</a><ul><li><a href=#links aria-label=Links>Links</a></li></ul></li></ul></div></details></div><div class=post-content><hr><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>This post encapsulates the work I did with Dean Fulgoni for our final project in Big Data Analytics. Credit to Professor Chris Callison-Burch for steering me away from some topics that &ldquo;could easily be Phd theses&rdquo; and to Professor Zachary Ives for being an excellent teacher.</p><h2 id=the-problem-we-tried-to-solve>The Problem We Tried to Solve<a hidden class=anchor aria-hidden=true href=#the-problem-we-tried-to-solve>#</a></h2><p>The problem is deviously simple - given two input questions, tell whether the questions ask the same thing (i.e. seek to reveal the same previously unknown knowledge). Not to get off track, but this begs some thought on the nature of what a question is and whether a question is separable from its posed context. See the [Stanford Encyclopedia of Philosophy (SEP)]<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> for an excellent review of the philosophy of questions. I won&rsquo;t be discussing it in detail, but my view on this problem is colored by the philosophical understanding of questions. We can assume that questions on Quora are requesting information, not testing the knowledge of the user base.</p><blockquote><p>&mldr; a question [is] an abstract thing for which an interrogative sentence is a piece of notation. (Belnap and Steel, 1976)</p></blockquote><p>The challenge was posed as a supervised learning question with a human labeled training set. A cursory glance at the training set showed that the human labeling was inconsistent, with varying standards to classify questions as semantically the same.</p><p>These were labeled as asking two different questions:</p><ul><li>Do we need smart cities or smart politicians?</li><li>Do we need smart cities or smart laws?</li></ul><p>We can see that the classification of this pair is debatable. Politicians are not laws. However, politicians are the only people with the power to make laws. Where is the line drawn for semantic similarity? How do we even approach situations where the the map between language and abstract meaning is complex?</p><p>$$
\begin{aligned}
Politician &\ne Law \cr
\\ Politician &\rightarrow Law \cr
\end{aligned}
$$</p><p>With an unclear standard for determining whether two questions had the same intent, we decided to approach the problem &rsquo;loosely&rsquo;. Overfitting in any specific approach would kill our performance on the test set. We partitioned the feature set into <strong>syntactic features</strong> and <strong>semantic features</strong>. I&rsquo;ll touch on semantic features later, but I decided to focus on syntactic features because it was much lower hanging fruit. The question became how to quantify the syntactic similarity of two sentences.</p><p>$$ f: (sentence_1, sentence_2) \rightarrow s \in [0,1] $$</p><p>I&rsquo;ll save the complete literature review for a separate post. We chose to use Tree Kernels championed by Prof. Alessandro Moschitti of the University of Trento, Italy because of the elegance of the theoretical basis.</p><h2 id=syntactic-tree-kernel-theory>Syntactic Tree Kernel Theory<a hidden class=anchor aria-hidden=true href=#syntactic-tree-kernel-theory>#</a></h2><p>As part of our search for unique features to encode syntactic and semantic similarity, we found that so called Tree Kernels had been shown to be highly effective in augmenting SVMs designed to solve problems in question classification, question answering, Semantic Role Labeling, and named entity recognition. The broad idea behind the theory is that syntactic features must be quantified to learn semantic structures. It isn&rsquo;t enough to say, compare lists of POS tags for two sentences because the structure of the sentences shape their meanings. We therefore design kernels that map from tree structures (e.g. constituency parse trees) to scores that measure the syntactic similarity of the two trees. Note that these scores are bounded for use in an SVM framework, but could easily be used as features in a deep learning framework.</p><h3 id=kernel-definition>Kernel Definition<a hidden class=anchor aria-hidden=true href=#kernel-definition>#</a></h3><p><em>The next few section are taken almost verbatim from Moschitti&rsquo;s ACL 2012 tutorial on State-of-the-art Kernels in Natural Language Processing. All credit goes to him and his colleagues.</em></p><p>The point of the following math is that we can ensure our SVM kernel is valid only if the matrix of transformed points is positive semi-definite. You can review the literature for the proof.</p><p>A <strong>kernel</strong> is a function:</p><p>$$ k: (\overrightarrow{x},\overrightarrow{z}) \rightarrow \Phi(\overrightarrow{x}) \cdot \Phi(\overrightarrow{z}) ~~~~~~\forall \overrightarrow{x}, \overrightarrow{z} \in X$$</p><p>Mercer&rsquo;s Conditions for a valid kernel:</p><p>Let $X$ be a finite input space and let $K(x,z)$ be a symmetric function on $X$. Then $K(x,z)$ is a kernel function IFF matrix:</p><p>$$ k(x,z) = \Phi(x) \cdot \Phi(z)$$</p><p>is positive semi-definite (non-negative eigenvalues). That is, if the matrix is positive semi-definite we can find a $\Phi$ that implements the kernel function $k$.</p><h2 id=tree-definition>Tree Definition<a hidden class=anchor aria-hidden=true href=#tree-definition>#</a></h2><p>We worked exclusively with <strong>constituency parse trees</strong>. Anywhere I write &ldquo;tree&rdquo;, I am referring to a constituency parse tree. A constituency parse tree breaks down a sentence into phrases using a well-defined, <strong>context-free</strong>, phrase structure grammar. A context-free grammar is a set of rules that describes all possible strings in a formal language. Given any sentence, we have a formal way to define that sentence as made up of phrases, nouns, verbs, etc. Noam Chomsky was the first define context-free grammars in linguistics to describe the universal structure of natural language. We use the Penn-Treebank II formal grammar for this project. Here is a small sample of the Penn-Treebank II formal grammar:</p><ul><li><strong>RB</strong>: Adverb</li><li><strong>IN</strong>: Preposition</li><li><strong>PP</strong>: Prepositional Phrase</li><li><strong>VP</strong>: Verb Phrase</li></ul><p>Below is an example of a constituency parse tree for the sentence:</p><blockquote><p>I will never stop learning.</p></blockquote><p>Notice that sentence tokens (the words) are always leaves of the tree and in their natural language order. That is, we should always be able to read the tree leaves from left to right and have our exact sentence.</p><p><img loading=lazy src=/assets/posts/TreeKernel/ConstituencyTree.jpg alt="Constituency Parse Tree"></p><h3 id=tree-kernel-core-theory-and-definitions>Tree Kernel Core Theory and Definitions<a hidden class=anchor aria-hidden=true href=#tree-kernel-core-theory-and-definitions>#</a></h3><p>A tree kernel is a function: $f: (tree_1, tree_2) \rightarrow \mathbf{R}^+ $</p><p>A tree is defined recursively as a set of nodes, each of which has a value and a set of children, each of which are trees. A node is a &ldquo;leaf&rdquo; if it has no children. A node is &ldquo;pre-terminal&rdquo; if all of its child nodes are leaves. The &ldquo;production&rdquo; of a node is the subtree containing the node and its children. The production of two nodes are the same if the nodes have the same value and their children have the same value. Because we work are working with ordered trees, the children must have the same values in the same order to be part of the same production.</p><p>Key to the effectiveness of tree kernel features in solving problems is the choice of tree structure. Moschitti shows that different tree kernels are effective for different choices of syntax trees in solving different classes of problems. We used Stanford NLP to generate parse trees from the input sentences. A parse tree is an ordered tree that represents the syntax of a sentence according to some context-free grammar. There are two main classes of parse trees: constituency and dependency. A dependency parse trees is the tree structure generated by the dependencies of a sentence. While dependencies can be highly useful in generating features, as described in Section 4.1, Moschitti found that constituency parse trees were most effective in solving classification related problems. A constituency parse tree breaks down the tokens of sentence into their constituent grammars.</p><h3 id=syntactic-tree-kernel-or-collins-duffy-stk>Syntactic Tree Kernel or &ldquo;Collins-Duffy&rdquo; (STK)<a hidden class=anchor aria-hidden=true href=#syntactic-tree-kernel-or-collins-duffy-stk>#</a></h3><p>For our tree kernel features we only implemented the Collins-Duffy tree kernel. The general idea of the Collins-Duffy Tree Kernel is that we want to compare the number of common subtrees between two input trees. The naive approach in exponential time is to enumerate all possible subtrees of the two input trees and then perform the dot-product. Obviously generating every subtree of each sentence, and then taking the dot-product of all of the subtrees to find how many the two sentences have in common would be a computationally nightmarish $O(n^2)$. We can take advantage of the fact that generating subtrees is recursive, and that if two parent nodes do not match, the subtrees rooted at those two nodes do not match. Collins and Duffy propose the following derivation.</p><p>Define the tree kernel $K: f(T_1,T_2) \rightarrow \mathbf{R}^+$ where $T_1$ and $T_2$ are well-formed trees. The dot-product approach above is written:</p><p>$$K(T_1,T_2) = h(T_1) \cdot h(T_2)$$</p><p>Where $h$ is a function that produces all of the subtrees of a tree. Again, the dot-product between $h(T_1)$ and $h(T_2)$ will count all of the subtrees $T_1$ and $T_2$ have in common. They define the indicator function $I_i(n)$ to be 1 if the subtree $i$ is rooted at node $n$ and 0 otherwise. Therefore, we can rewrite the dot-product above.</p><p>$$
\begin{aligned}
h(T_1) \cdot h(T_2) &= \sum_i h_i(T_1)h_i(T_2)
\\ & = \sum_{n_1 \in N_1}\sum_{n_2 \in N_2} \sum_i I_i(n_1) I_i(n_2)
\\ & = \sum_{n_1 \in N_1}\sum_{n_2 \in N_2} C(n_1, n_2)
\end{aligned}
$$</p><p>Where $C(n_1,n_2)$ is defined as $\sum_i I_i(n_1)I_i(n_2)$. We note that $C(n_1, n_2)$ can be computed recursively in polynomial time:</p><p>$C(n_1,n_2) = 0$ if the productions at $n_1$ and $n_2$ are different.</p><p>$C(n_1,n_2) = \lambda$ if the productions at $n_1$ and $n_2$ are the same and $n_1$ and $n_2$ are pre-terminal.</p><p>$$C(n_1,n_2) = \lambda \prod_{j=1}^{nc(n_1)}(\sigma + C(ch(n_1,j),ch(n_2,j)))$$</p><p>Where $0 &lt; \lambda \leq 1$ is a factor that down weights subtrees exponentially with their size, $nc(n_i)$ returns the number of children of node $i$, binary variable $\sigma \in {0,1}$ determines whether we use the ST or SST kernel, and $ch(n_i,j)$ selects the $j$th child of node $i$. The ST or subtree kernel is where $\sigma = 0$. For the ST kernel, the entire subtree must match to return a non-zero score. The SST or subset tree kernel is where $\sigma = 1$. For the SST kernel, non-zero scores can be achieved even if all of the subtrees of two nodes do not match.</p><p>The distinction between the ST and SST kernels in the previous section is that the SST kernel is more forgiving than the ST kernel. The SST kernel can still return high scores even if the exact sentences are not the same. Likewise, the ST kernel heavily penalizes minor differences between constituency trees. The factor $\lambda$ can be viewed as a penalty we utilize so that kernel does not over-inflate the importance of matches between larger subtrees. These parameters become less important when we normalize the kernel score using:</p><p>$$K_{norm}(T_1,T_2) = \frac{K(T_1,T_2)}{K(T_1,T_1)K(T_2,T_2)}</p><p>We generated features with the ST and SST tree kernels for each question pair using $\lambda \in {1, 0.8, 0.5, 0.2, 0.1, 0.05, 0.2}$ totaling 14 tree kernel features. Determining the optimal $\lambda$ for this task would be a good exploration on its own.</p><h2 id=synactic-tree-kernels-in-python>Synactic Tree Kernels in Python<a hidden class=anchor aria-hidden=true href=#synactic-tree-kernels-in-python>#</a></h2><p>As far as I know, this is the first implementation of syntactic tree kernels in Python. This code may eventually move to its own maintained repository if it&rsquo;s needed. If you find any bugs please reach out to me. Dr. Moschitti&rsquo;s cited implementation is:</p><p>[SVM-Light]<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> written by Thorsten Joachims in C. His implementation is most-likely faster (no comparison benchmark yet). My implementation is however easier to grasp and is isolated from a complete SVM library.</p><h3 id=building-the-parse-tree>Building the Parse Tree<a hidden class=anchor aria-hidden=true href=#building-the-parse-tree>#</a></h3><p>As described in the Pipeline section we used Stanford CoreNLP to generate our constituency parse trees. There aren&rsquo;t many libraries that are good for constituency parsing, and CoreNLP is by far the most stable and customizable. Oddly enough, Stanford CoreNLP will print a constituency parse tree for a given sentence with well-defined structure to console (i.e. pretty print), but provides no functionality to store the tree in a reasonable format (linked list, hashmap, etc. ). The code in this subsection is available on my [Github]<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> and is used to take the raw text output of Stanford CoreNLP and store in the schema below for tree kernel computation. We label each token with an Id and store the tokens for fast computation, but admittedly poor space complexity.</p><ul><li>curid:    <em>Integer</em></li><li>parid:    <em>Integer</em></li><li>posOrTok:  <em>String</em></li><li>indent:    <em>Integer</em></li><li>children:    <em>[Integer]</em></li><li>childrenTok:  <em>[String]</em></li></ul><h3 id=computing-tree-kernels>Computing Tree Kernels<a hidden class=anchor aria-hidden=true href=#computing-tree-kernels>#</a></h3><div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">  9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 46
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 47
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 48
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 49
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 50
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 51
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 52
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 53
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 54
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 55
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 56
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 57
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 58
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 59
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 60
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 61
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 62
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 63
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 64
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 65
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 66
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 67
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 68
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 69
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 70
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 71
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 72
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 73
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 74
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 75
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 76
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 77
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 78
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 79
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 80
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 81
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 82
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 83
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 84
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 85
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 86
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 87
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 88
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 89
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 90
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 91
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 92
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 93
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 94
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 95
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 96
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 97
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 98
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 99
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">100
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">101
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">102
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">103
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">104
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">105
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">106
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">107
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">108
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">109
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">110
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">111
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">112
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">113
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">114
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">115
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">116
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">117
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">118
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">119
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">120
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">121
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">122
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">123
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">124
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">125
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">126
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">127
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">128
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">129
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">130
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">131
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">132
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">133
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">134
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">135
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">136
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">137
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">138
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">139
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">140
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">141
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">142
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">143
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">144
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">145
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">146
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">147
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">148
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">149
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">150
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">151
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">152
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">153
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">154
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">155
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">156
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">157
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">158
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">159
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">160
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">161
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">162
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">163
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">164
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">165
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">166
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">167
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">168
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">169
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">170
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">171
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">172
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">173
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">174
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">175
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">176
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">177
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">178
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">179
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">180
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">181
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">182
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">183
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">184
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> numpy <span style=color:#5f8700>as</span> np
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> math
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> nltk.corpus <span style=color:#d75f00>import</span> wordnet <span style=color:#5f8700>as</span> wn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>Helper Methods
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_isLeaf_</span>(tree, parentNode):
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> (<span style=color:#0087ff>len</span>(tree[parentNode][<span style=color:#00afaf>&#39;children&#39;</span>]) == <span style=color:#00afaf>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_isPreterminal_</span>(tree, parentNode):
</span></span><span style=display:flex><span>    <span style=color:#5f8700>for</span> idx <span style=color:#5f8700>in</span> tree[parentNode][<span style=color:#00afaf>&#39;children&#39;</span>]:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>if</span> <span style=color:#5f8700>not</span> _isLeaf_(tree, idx):
</span></span><span style=display:flex><span>            <span style=color:#5f8700>return</span> <span style=color:#d75f00>False</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> <span style=color:#d75f00>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>Implementation of the Colins-Duffy or Subset-Tree (SST) Kernel
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_cdHelper_</span>(tree1, tree2, node1, node2, store, lam, SST_ON):
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># No duplicate computations</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> store[node1, node2] &gt;= <span style=color:#00afaf>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Leaves yield similarity score by definition</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> (_isLeaf_(tree1, node1) <span style=color:#5f8700>or</span> _isLeaf_(tree2, node2)):
</span></span><span style=display:flex><span>        store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># same parent node</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> tree1[node1][<span style=color:#00afaf>&#39;posOrTok&#39;</span>] == tree2[node2][<span style=color:#00afaf>&#39;posOrTok&#39;</span>]:
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># same children tokens</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>if</span> tree1[node1][<span style=color:#00afaf>&#39;childrenTok&#39;</span>] == tree2[node2][<span style=color:#00afaf>&#39;childrenTok&#39;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#4e4e4e># Check if both nodes are pre-terminal</span>
</span></span><span style=display:flex><span>            <span style=color:#5f8700>if</span> _isPreterminal_(tree1, node1) <span style=color:#5f8700>and</span> _isPreterminal_(tree2, node2):
</span></span><span style=display:flex><span>                store[node1, node2] = lam
</span></span><span style=display:flex><span>                <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>            <span style=color:#4e4e4e># Not pre-terminal. Recurse among the children of both token trees.</span>
</span></span><span style=display:flex><span>            <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>                nChildren = <span style=color:#0087ff>len</span>(tree1[node1][<span style=color:#00afaf>&#39;children&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                runningTotal = <span style=color:#d75f00>None</span>
</span></span><span style=display:flex><span>                <span style=color:#5f8700>for</span> idx <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(nChildren):
</span></span><span style=display:flex><span>                     <span style=color:#4e4e4e># index -&gt;  node_id</span>
</span></span><span style=display:flex><span>                    tmp_n1 = tree1[node1][<span style=color:#00afaf>&#39;children&#39;</span>][idx]
</span></span><span style=display:flex><span>                    tmp_n2 = tree2[node2][<span style=color:#00afaf>&#39;children&#39;</span>][idx]
</span></span><span style=display:flex><span>                    <span style=color:#4e4e4e># Recursively run helper</span>
</span></span><span style=display:flex><span>                    _cdHelper_(tree1, tree2, tmp_n1, tmp_n2, store, lam, SST_ON)
</span></span><span style=display:flex><span>                    <span style=color:#4e4e4e># Set the initial value for the layer. Else multiplicative product.</span>
</span></span><span style=display:flex><span>                    <span style=color:#5f8700>if</span> (runningTotal == <span style=color:#d75f00>None</span>):
</span></span><span style=display:flex><span>                        runningTotal = SST_ON + store[tmp_n1, tmp_n2]
</span></span><span style=display:flex><span>                    <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>                        runningTotal *= (SST_ON + store[tmp_n1, tmp_n2])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                store[node1, node2] = lam * runningTotal
</span></span><span style=display:flex><span>                <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>            store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>else</span>: <span style=color:#4e4e4e># parent nodes are different</span>
</span></span><span style=display:flex><span>        store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_cdKernel_</span>(tree1, tree2, lam, SST_ON):
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Fill the initial state of the store</span>
</span></span><span style=display:flex><span>    store = np.empty((<span style=color:#0087ff>len</span>(tree1), <span style=color:#0087ff>len</span>(tree2)))
</span></span><span style=display:flex><span>    store.fill(-<span style=color:#00afaf>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># O(N^2) to compute the tree dot product</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>for</span> i <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#0087ff>len</span>(tree1)):
</span></span><span style=display:flex><span>        <span style=color:#5f8700>for</span> j <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#0087ff>len</span>(tree2)):
</span></span><span style=display:flex><span>            _cdHelper_(tree1, tree2, i, j, store, lam, SST_ON)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> store.sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>Returns a tuple w/ format: (raw, normalized)
</span></span></span><span style=display:flex><span><span style=color:#00afaf>If NORMALIZE_FLAG set to False, tuple[1] = -1
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_CollinsDuffy_</span>(tree1, tree2, lam, NORMALIZE_FLAG, SST_ON):
</span></span><span style=display:flex><span>    raw_score = _cdKernel_(tree1, tree2, lam, SST_ON)
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> (NORMALIZE_FLAG):
</span></span><span style=display:flex><span>        t1_score = _cdKernel_(tree1, tree1, lam, SST_ON)
</span></span><span style=display:flex><span>        t2_score = _cdKernel_(tree2, tree2, lam, SST_ON)
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> (raw_score,(raw_score / math.sqrt(t1_score * t2_score)))
</span></span><span style=display:flex><span>    <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> (raw_score,-<span style=color:#00afaf>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>Implementation of the Partial Tree Kernel (PTK) from:
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#34;Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees&#34;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>by Alessandro Moschitti
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>The delta function is stolen from the Collins-Duffy kernel
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_deltaP_</span>(tree1, tree2, seq1, seq2, store, lam, mu, p):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#4e4e4e>#     # Enumerate subsequences of length p+1 for each child set</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> p == <span style=color:#00afaf>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># generate delta(a,b)</span>
</span></span><span style=display:flex><span>        _delta_(tree1, tree2, seq1[-<span style=color:#00afaf>1</span>], seq2[-<span style=color:#00afaf>1</span>], store, lam, mu)
</span></span><span style=display:flex><span>        <span style=color:#5f8700>if</span> store[seq1[-<span style=color:#00afaf>1</span>], seq2[-<span style=color:#00afaf>1</span>]] == <span style=color:#00afaf>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#5f8700>return</span> <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>            runningTot = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>            <span style=color:#5f8700>for</span> i <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(p-<span style=color:#00afaf>1</span>, <span style=color:#0087ff>len</span>(seq1)-<span style=color:#00afaf>1</span>):
</span></span><span style=display:flex><span>                <span style=color:#5f8700>for</span> r <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(p-<span style=color:#00afaf>1</span>, <span style=color:#0087ff>len</span>(seq2)-<span style=color:#00afaf>1</span>):
</span></span><span style=display:flex><span>                    scaleFactor = <span style=color:#0087ff>pow</span>(lam, <span style=color:#0087ff>len</span>(seq1[:-<span style=color:#00afaf>1</span>])-i+<span style=color:#0087ff>len</span>(seq2[:-<span style=color:#00afaf>1</span>])-r)
</span></span><span style=display:flex><span>                    dp = _deltaP_(tree1, tree2, seq1[:i], seq2[:r], store, lam, mu, p-<span style=color:#00afaf>1</span>)
</span></span><span style=display:flex><span>                    runningTot += (scaleFactor * dp)
</span></span><span style=display:flex><span>            <span style=color:#5f8700>return</span> runningTot
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_delta_</span>(tree1, tree2, node1, node2, store, lam, mu):
</span></span><span style=display:flex><span><span style=color:#4e4e4e>#     print(&#34;Evaluating Delta: (%d,%d)&#34; % (node1, node2))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># No duplicate computations</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> store[node1, node2] &gt;= <span style=color:#00afaf>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Leaves yield similarity score by definition</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> (_isLeaf_(tree1, node1) <span style=color:#5f8700>or</span> _isLeaf_(tree2, node2)):
</span></span><span style=display:flex><span>        store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># same parent node</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> tree1[node1][<span style=color:#00afaf>&#39;posOrTok&#39;</span>] == tree2[node2][<span style=color:#00afaf>&#39;posOrTok&#39;</span>]:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>if</span> _isPreterminal_(tree1, node1) <span style=color:#5f8700>and</span> _isPreterminal_(tree2, node2):
</span></span><span style=display:flex><span>            <span style=color:#5f8700>if</span> tree1[node1][<span style=color:#00afaf>&#39;childrenTok&#39;</span>] == tree2[node2][<span style=color:#00afaf>&#39;childrenTok&#39;</span>]:
</span></span><span style=display:flex><span>                store[node1, node2] = lam
</span></span><span style=display:flex><span>            <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>                store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>            <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#4e4e4e># establishes p_max</span>
</span></span><span style=display:flex><span>            childmin = <span style=color:#0087ff>min</span>(<span style=color:#0087ff>len</span>(tree1[node1][<span style=color:#00afaf>&#39;children&#39;</span>]), <span style=color:#0087ff>len</span>(tree2[node2][<span style=color:#00afaf>&#39;children&#39;</span>]))
</span></span><span style=display:flex><span>            deltaTot = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>            <span style=color:#5f8700>for</span> p <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#00afaf>1</span>,childmin+<span style=color:#00afaf>1</span>):
</span></span><span style=display:flex><span>                <span style=color:#4e4e4e># compute delta_p</span>
</span></span><span style=display:flex><span>                deltaTot += _deltaP_(tree1, tree2,
</span></span><span style=display:flex><span>                                     tree1[node1][<span style=color:#00afaf>&#39;children&#39;</span>],
</span></span><span style=display:flex><span>                                     tree2[node2][<span style=color:#00afaf>&#39;children&#39;</span>], store, lam, mu, p)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            store[node1, node2] = mu * (<span style=color:#0087ff>pow</span>(lam,<span style=color:#00afaf>2</span>) + deltaTot)
</span></span><span style=display:flex><span>            <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># parent nodes are different</span>
</span></span><span style=display:flex><span>        store[node1, node2] = <span style=color:#00afaf>0</span>
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_ptKernel_</span>(tree1, tree2, lam, mu):
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># Fill the initial state of the store</span>
</span></span><span style=display:flex><span>    store = np.empty((<span style=color:#0087ff>len</span>(tree1), <span style=color:#0087ff>len</span>(tree2)))
</span></span><span style=display:flex><span>    store.fill(-<span style=color:#00afaf>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e># O(N^2) to compute the tree dot product</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>for</span> i <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#0087ff>len</span>(tree1)):
</span></span><span style=display:flex><span>        <span style=color:#5f8700>for</span> j <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#0087ff>len</span>(tree2)):
</span></span><span style=display:flex><span>            _delta_(tree1, tree2, i, j, store, lam, mu)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>return</span> store.sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;
</span></span></span><span style=display:flex><span><span style=color:#00afaf>Returns a tuple w/ format: (raw, normalized)
</span></span></span><span style=display:flex><span><span style=color:#00afaf>If NORMALIZE_FLAG set to False, tuple[1] = -1
</span></span></span><span style=display:flex><span><span style=color:#00afaf>&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span><span style=color:#5f8700>def</span> <span style=color:#0087ff>_MoschittiPT_</span>(tree1, tree2, lam, mu, NORMALIZE_FLAG):
</span></span><span style=display:flex><span>    raw_score = _ptKernel_(tree1, tree2, lam, mu)
</span></span><span style=display:flex><span>    <span style=color:#5f8700>if</span> (NORMALIZE_FLAG):
</span></span><span style=display:flex><span>        t1_score = _ptKernel_(tree1, tree1, lam, mu)
</span></span><span style=display:flex><span>        t2_score = _ptKernel_(tree2, tree2, lam, mu)
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> (raw_score,(raw_core / math.sqrt(t1_score * t2_score)))
</span></span><span style=display:flex><span>    <span style=color:#5f8700>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> (raw_score,-<span style=color:#00afaf>1</span>)
</span></span></code></pre></td></tr></table></div></div><h2 id=towards-syntactic-semantic-tree-kernels>Towards Syntactic-Semantic Tree Kernels<a hidden class=anchor aria-hidden=true href=#towards-syntactic-semantic-tree-kernels>#</a></h2><p>Ignoring all of the theory and code in the previous sections would not be unreasonable. The major criticism of ST and PTK kernels is that they are syntactic, and syntax is not meaning. A simple example:</p><ol><li>The girl went to the store to buy <strong>eggs</strong> for the family.</li><li>The girl went to the store to buy <strong>milk</strong> for the family.</li></ol><p>The sentences generate the same constituency parse tree (minus) the differing terminal leaf, but have completely different meanings. Put plainly, the sentences above are grammatically the same except for the one word difference. However, the SST kernel we implemented above would give a very high similarity score for the <strong>meaning</strong> of the two sentences even though we know that they have different meanings. Bloehdorn and Moschitti propose a theoretically elegant modification to solve our problem. This new syntactic-semantic tree kernel (<strong>SSTK</strong>) is defined as:</p><p>$$ K_{SSTK} = comp(f_1, f_2) \prod_{t=1}^{nt_{(f1)}} SIM(f_1(t),f_2(t)) $$</p><p>where $comp(f_1, f_2)$ is $1$ if $f_1$ differs from $f_2$ only in the terminal nodes and 0 otherwise, $nt(f_i)$ is the number of terminal nodes and $f_i(t)$ is the t-th terminal symbol of $f_i$ numbered from left to right, and $SIM$ is a function returning $[0, 1]$ where $0$ is completely different, and $1$ means the words are identical in meaning.</p><p>Now in plain English. If the two tree fragments don&rsquo;t have the same non-terminal structure then we end the computation early. The SSTK is the same as PTK and SST in that it ignores tree fragments that are not <strong>identical</strong> in syntactic structure (grammar). Say the two tree fragments have the same non-terminal structure, but different terminal nodes (leaves). This means the two sentence structures have the same syntax but different words. So our indicator function $comp(f_1, f_2)$ returns 1, and we compute the multiplicative product by computing the similarity of each in-order pair of terminal leaves.</p><p>This is where it gets admittedly ridiculous. How exactly do we quantify the semantic similarity of two words. Doesn&rsquo;t this depend on the sentence context? This is where there is no good answer yet. There are computational kernels that give similarity metrics on word databases like WordNet, but we don&rsquo;t have significant evidence on which is best for different classes of problems. Our implementation used the raw score returned by WordNet, but that isn&rsquo;t very interesting.</p><h2 id=lesson-learned-hardly-earned>Lesson Learned, Hardly Earned<a hidden class=anchor aria-hidden=true href=#lesson-learned-hardly-earned>#</a></h2><p>Research tools like StanfordCoreNLP are not performant. Most my time was spent building tooling, testing my implementations of paper algorithms, running batch jobs on AWS instances, debugging the batch processing jobs on AWS, and questioning my sanity for embarking on this rabbit hole.</p><h2 id=if-we-only-had-more-time>If We Only Had More Time<a hidden class=anchor aria-hidden=true href=#if-we-only-had-more-time>#</a></h2><p>I would have worked on:</p><p>[1] Reworking this kernel as a feature generator in a deep learning approach. Consider that the behavior of this entire field is based around using these as a mathematically valid kernel in an SVM. Deep learning has showed greater promise in solving these types of problems because the neural network structure is more adaptable.</p><p>[2] Grammars: A Google Trends search shows >85% Quora’s user base can be divided as living on the Indian subcontinent and the United States, yet English is almost exclusively the only language used to ask/answer questions. This matters because we should not expect perfect English grammar from people whose first language isn’t English. Any effective model should detect common misuses of English grammar specific to users from the Indian subcontinent and subsequently &ldquo;loosen&rdquo;. Note that this is different from detecting common colloquial expressions or slang that native English speakers use.</p><p>[3] More work on incorporating different word similarity databases into the SSTK kernel. This could be anything from Wordnet, to scraped data from forums, to the paraphrase database (PPDB) at UPenn.</p><p>[4] Using different metrics for WordNet semantic similarities like Resnik, Wu & Palmer, or Lin.</p><h3 id=links>Links<a hidden class=anchor aria-hidden=true href=#links>#</a></h3><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://plato.stanford.edu/entries/questions/>https://plato.stanford.edu/entries/questions/</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=http://svmlight.joachims.org/>http://svmlight.joachims.org/</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/goodmattg/quora_kaggle/blob/master/TreeBuild.py>https://github.com/goodmattg/quora_kaggle/blob/master/TreeBuild.py</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://goodmattg.github.io>Matt's Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>