<!doctype html><html><head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>Relational Inductive Biases: a manifesto</title>
<meta name=description content="The website of Matthew Goodman">
<meta name=author content="Matthew Goodman">
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css?family=Roboto" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous>
<link rel=stylesheet href=/sass/researcher.min.css>
<link rel=icon type=image/ico href=https://goodmattg.github.io/favicon.ico>
</head>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<body><div class="container mt-5">
<nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
<a class="navbar-brand mx-0 mr-sm-auto" href=https://goodmattg.github.io>
Matt Goodman
</a>
<div class="navbar-nav flex-row flex-wrap justify-content-center">
<a class="nav-item nav-link" href=/about>
About
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/posts>
Posts
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/bookshelf>
Bookshelf
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/quotes>
Quotes
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/contact>
Contact
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/sync-motion>
SyncMotion
</a>
</div>
</nav>
</div>
<hr>
<div id=content>
<div class=container>
<div style=text-align:center class=centered-div>
<h1 id=relational-inductive-biases-a-manifesto>Relational Inductive Biases: a manifesto</h1>
<hr>
<br>
</div>
<p>The authors present a succinct and powerful manifesto of the importance of graph networks in artificial intelligence research. The paper is a move to synthesize the previously disjoint field of graph neural networks into a single framework: &ldquo;Graph Networks (GN)&rdquo;.</p>
<h1 id=thoughts>Thoughts</h1>
<p>We have a much needed survey paper that unifies graph networks (GNs) and refocuses attention away from the technical details of network composition/functionality back to the marquee goal of building networks that achieve combinatorial generalization through flexible computation on structured representation. This topic aligns strongly with my research interests and made me consider a few areas to explore moving forward:</p>
<ul>
<li>Recurrent GNs for sequences of graphs</li>
<li>Reinforcement learning as a method to adaptively modify graph structures during the course of computation. This would solve a personal thought experiment of training a self-aware image classifier that has the means to create new buckets. This would involve an update function that has the capacity to add/remove new buckets to the image classifier based on some spectral representation of the class output probabilities. In the formalism of GN, this is creating update/aggregation functions that can create/destroy nodes and edges.</li>
</ul>
<h1 id=quotes>Quotes</h1>
<blockquote>
<p>We suggest that a key path forward for modern AI is to commit to combinatorial generalization as a top priority.</p>
</blockquote>
<blockquote>
<p>When learning, we either fit new knowledge into our existing structured representations, or adjust the structure itself to better accommodate (and make use of) the new and the old.</p>
</blockquote>
<blockquote>
<p>Combinatorial generalization can be viewed as &ldquo;the infinite use of finite means&rdquo; - von Humboldt</p>
</blockquote>
<h1 id=notes>Notes</h1>
<ul>
<li>prioritize &ldquo;combinatorial generalization&rdquo;, the ability to generate new inferences, predictions, and behaviors from know building blocks
<ul>
<li>i.e. <em>putting the pieces together</em></li>
</ul>
</li>
<li>combinatorial generalization is the goal, so we should bias towards learning on structured representations and computations
<ul>
<li>Generalizing on structured representations is easier than generalizing on unstructured representations</li>
</ul>
</li>
<li>The world is <em>compositional</em>, therefore our methods should be compositional
<ul>
<li>This does not mean we mean to trade off structure for flexibility. We consider the concepts jointly (i.e. nature <em>and</em> nurture)</li>
</ul>
</li>
<li>Graph neural networks (GNs) exemplify this functional ambition by learning relations between &ldquo;explicitly structured data&rdquo;
<ul>
<li>learning $\rightarrow$ flexible</li>
<li>structured $\rightarrow$ composing known building blocks</li>
</ul>
</li>
<li>GN methods contain strong <strong>relational inductive biases</strong>
<ol>
<li>Relational reasoning
<ul>
<li><em>structure</em> is composing a set of known building blocks</li>
<li><em>structured representation</em> describe this composition</li>
<li>structured representations consist of <em>entities</em> connected by <em>relations</em> according to <em>rules</em>
<ul>
<li><em>entity</em> is an element with attributes</li>
<li><em>relation</em> is a property between entities</li>
<li><em>rule</em> is a function that maps discrete pairs of entity-relations</li>
</ul>
</li>
<li><em>relational reasoning</em> involves manipulating structured representations</li>
</ul>
</li>
<li>Inductive biases
<ul>
<li>An <em>inductive bias</em> allows a learning algorithm to prioritize one solution over another, independent of the observed data.
<ul>
<li>E.g. prior in Bayesian statistics, regularization terms, algorithm architecture, composition of layers in NN, etc.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Relational Inductive Biases</strong>
<ul>
<li>&ldquo;Inductive biases which impose constraints on relationships and interactions among entities in a learning process&rdquo;</li>
</ul>
</li>
</ol>
</li>
<li><em>Graph Networks</em> (GN)
<ul>
<li>$G = (u, V, E)$
<ul>
<li><strong>u</strong>: Global attribute</li>
<li><strong>V</strong> is the set of nodes w/ cardinality $N^v$ where each node $v_i$ has attributes</li>
<li><strong>E</strong> is the set of edges w/ cardinality $N^e$ where each edge $e_v$ has attributes. $E = {(e_k,r_k,s_k)}_{k=1:N^{e}}$
<ul>
<li>$e_k$ is the edge&rsquo;s attribute</li>
<li>$r_k$ is the index of the receiver node</li>
<li>$s_k$ is the index of the sender node</li>
</ul>
</li>
</ul>
</li>
<li>GN <em>block</em> := $f: G \rightarrow G$
<ul>
<li>GN block algorithm
<ol>
<li>Compute updated edge attributes: $e_k^{'} = \phi^e(e_k,v_{r_k},v_{s_k},u)$</li>
<li>Aggregate edge attributes per node: $\bar{e}_i^{'} = \rho^{e \rightarrow v}(E_i^{'})$</li>
<li>Compute updated node attributes: $v_i^{'} = \phi^v(\bar{e}_i^{'}, v_i, u)$</li>
<li>Aggregate edge attributes globally: $\bar{e}^{'} = \rho^{e \rightarrow u}(E^{'})$</li>
<li>Aggregate node attributes globally: $\bar{v}_i^{'} = \rho^{v \rightarrow u}(V^{'})$</li>
<li>Compute updated global attribute: $u^{'} = \phi^u(\bar{e}^{'}, \bar{v}^{'}, u)$</li>
</ol>
</li>
<li>TLDR
<ul>
<li>Update functions: $\phi$
<ul>
<li>SOA use NN as update function ($NN_e, NN_v, NN_u$)</li>
<li>Can include RNN as $\phi$, but this is not in literature yet</li>
</ul>
</li>
<li>Aggregation functions: $\rho$</li>
</ul>
</li>
</ul>
</li>
<li>Relational inductive biases in GN
<ul>
<li>arbitrary relationships among entities. i.e. the GN architecture doesn&rsquo;t determine how entities interact.</li>
<li>GN are invariant to ordering on input (b/c input is graph, and graph represented as set)</li>
<li>update/aggregation functions are reused implying combinatorial generalization</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id=cite>Cite</h2>
<p>[1] Battaglia, Peter W., et al. &ldquo;Relational inductive biases, deep learning, and graph networks.&rdquo; arXiv preprint arXiv:1806.01261 (2018)</p>
</div>
</div><div id=footer class=mb-5>
<script src=https://utteranc.es/client.js repo=goodmattg/goodmattg.github.io issue-term=pathname label=blog theme=github-light crossorigin=anonymous async></script>
<hr>
<div class="container text-center">
<a href=https://goodmattg.github.io><small>By Matthew Goodman</small></a>
</div>
</div>
</body>
</html>