<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Relational Inductive Biases: a manifesto | Matt's Log</title><meta name=keywords content><meta name=description content="DeepMind, Google Brain, MIT, and University of Edinburgh collaborators produce a manifesto on graph networks"><meta name=author content="Matt Goodman"><link rel=canonical href=https://goodmattg.github.io/posts/relation-inductive-biases-a-manifesto/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://goodmattg.github.io/magnet.ico><link rel=icon type=image/png sizes=16x16 href=https://goodmattg.github.io/magnet-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://goodmattg.github.io/magnet-32x32.ico><link rel=apple-touch-icon href=https://goodmattg.github.io/magnet-apple-touch-icon.png><link rel=mask-icon href=https://goodmattg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Relational Inductive Biases: a manifesto"><meta property="og:description" content="DeepMind, Google Brain, MIT, and University of Edinburgh collaborators produce a manifesto on graph networks"><meta property="og:type" content="article"><meta property="og:url" content="https://goodmattg.github.io/posts/relation-inductive-biases-a-manifesto/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-04-24T00:00:00+00:00"><meta property="article:modified_time" content="2019-04-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Relational Inductive Biases: a manifesto"><meta name=twitter:description content="DeepMind, Google Brain, MIT, and University of Edinburgh collaborators produce a manifesto on graph networks"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://goodmattg.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Relational Inductive Biases: a manifesto","item":"https://goodmattg.github.io/posts/relation-inductive-biases-a-manifesto/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Relational Inductive Biases: a manifesto","name":"Relational Inductive Biases: a manifesto","description":"DeepMind, Google Brain, MIT, and University of Edinburgh collaborators produce a manifesto on graph networks","keywords":[],"articleBody":" The authors present a succinct and powerful manifesto of the importance of graph networks in artificial intelligence research. The paper is a move to synthesize the previously disjoint field of graph neural networks into a single framework: “Graph Networks (GN)”.\nThoughts We have a much needed survey paper that unifies graph networks (GNs) and refocuses attention away from the technical details of network composition/functionality back to the marquee goal of building networks that achieve combinatorial generalization through flexible computation on structured representation. This topic aligns strongly with my research interests and made me consider a few areas to explore moving forward:\nRecurrent GNs for sequences of graphs Reinforcement learning as a method to adaptively modify graph structures during the course of computation. This would solve a personal thought experiment of training a self-aware image classifier that has the means to create new buckets. This would involve an update function that has the capacity to add/remove new buckets to the image classifier based on some spectral representation of the class output probabilities. In the formalism of GN, this is creating update/aggregation functions that can create/destroy nodes and edges. Quotes We suggest that a key path forward for modern AI is to commit to combinatorial generalization as a top priority.\nWhen learning, we either fit new knowledge into our existing structured representations, or adjust the structure itself to better accommodate (and make use of) the new and the old.\nCombinatorial generalization can be viewed as “the infinite use of finite means” - von Humboldt\nNotes prioritize “combinatorial generalization”, the ability to generate new inferences, predictions, and behaviors from know building blocks i.e. putting the pieces together combinatorial generalization is the goal, so we should bias towards learning on structured representations and computations Generalizing on structured representations is easier than generalizing on unstructured representations The world is compositional, therefore our methods should be compositional This does not mean we mean to trade off structure for flexibility. We consider the concepts jointly (i.e. nature and nurture) Graph neural networks (GNs) exemplify this functional ambition by learning relations between “explicitly structured data” learning $\\rightarrow$ flexible structured $\\rightarrow$ composing known building blocks GN methods contain strong relational inductive biases Relational reasoning structure is composing a set of known building blocks structured representation describe this composition structured representations consist of entities connected by relations according to rules entity is an element with attributes relation is a property between entities rule is a function that maps discrete pairs of entity-relations relational reasoning involves manipulating structured representations Inductive biases An inductive bias allows a learning algorithm to prioritize one solution over another, independent of the observed data. E.g. prior in Bayesian statistics, regularization terms, algorithm architecture, composition of layers in NN, etc. Relational Inductive Biases “Inductive biases which impose constraints on relationships and interactions among entities in a learning process” Graph Networks (GN) $G = (u, V, E)$ u: Global attribute V is the set of nodes w/ cardinality $N^v$ where each node $v_i$ has attributes E is the set of edges w/ cardinality $N^e$ where each edge $e_v$ has attributes. $E = {(e_k,r_k,s_k)}_{k=1:N^{e}}$ $e_k$ is the edge’s attribute $r_k$ is the index of the receiver node $s_k$ is the index of the sender node GN block := $f: G \\rightarrow G$ GN block algorithm Compute updated edge attributes: $e_k^{’} = \\phi^e(e_k,v_{r_k},v_{s_k},u)$ Aggregate edge attributes per node: $\\bar{e}_i^{’} = \\rho^{e \\rightarrow v}(E_i^{’})$ Compute updated node attributes: $v_i^{’} = \\phi^v(\\bar{e}_i^{’}, v_i, u)$ Aggregate edge attributes globally: $\\bar{e}^{’} = \\rho^{e \\rightarrow u}(E^{’})$ Aggregate node attributes globally: $\\bar{v}_i^{’} = \\rho^{v \\rightarrow u}(V^{’})$ Compute updated global attribute: $u^{’} = \\phi^u(\\bar{e}^{’}, \\bar{v}^{’}, u)$ TLDR Update functions: $\\phi$ SOA use NN as update function ($NN_e, NN_v, NN_u$) Can include RNN as $\\phi$, but this is not in literature yet Aggregation functions: $\\rho$ Relational inductive biases in GN arbitrary relationships among entities. i.e. the GN architecture doesn’t determine how entities interact. GN are invariant to ordering on input (b/c input is graph, and graph represented as set) update/aggregation functions are reused implying combinatorial generalization Cite [1] Battaglia, Peter W., et al. “Relational inductive biases, deep learning, and graph networks.” arXiv preprint arXiv:1806.01261 (2018)\n","wordCount":"687","inLanguage":"en","datePublished":"2019-04-24T00:00:00Z","dateModified":"2019-04-24T00:00:00Z","author":{"@type":"Person","name":"Matt Goodman"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://goodmattg.github.io/posts/relation-inductive-biases-a-manifesto/"},"publisher":{"@type":"Organization","name":"Matt's Log","logo":{"@type":"ImageObject","url":"https://goodmattg.github.io/magnet.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://goodmattg.github.io accesskey=h title="Matt's Log (Alt + H)">Matt's Log</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://goodmattg.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://goodmattg.github.io/about title=About><span>About</span></a></li><li><a href=https://goodmattg.github.io/bookshelf title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://goodmattg.github.io/quotes title=Quotes><span>Quotes</span></a></li><li><a href=https://goodmattg.github.io/contact title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Relational Inductive Biases: a manifesto</h1><div class=post-description>DeepMind, Google Brain, MIT, and University of Edinburgh collaborators produce a manifesto on graph networks</div><div class=post-meta><span title='2019-04-24 00:00:00 +0000 UTC'>April 24, 2019</span>&nbsp;·&nbsp;Matt Goodman</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#thoughts aria-label=Thoughts>Thoughts</a></li><li><a href=#quotes aria-label=Quotes>Quotes</a></li><li><a href=#notes aria-label=Notes>Notes</a><ul><li><a href=#cite aria-label=Cite>Cite</a></li></ul></li></ul></div></details></div><div class=post-content><hr><p>The authors present a succinct and powerful manifesto of the importance of graph networks in artificial intelligence research. The paper is a move to synthesize the previously disjoint field of graph neural networks into a single framework: &ldquo;Graph Networks (GN)&rdquo;.</p><h1 id=thoughts>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts>#</a></h1><p>We have a much needed survey paper that unifies graph networks (GNs) and refocuses attention away from the technical details of network composition/functionality back to the marquee goal of building networks that achieve combinatorial generalization through flexible computation on structured representation. This topic aligns strongly with my research interests and made me consider a few areas to explore moving forward:</p><ul><li>Recurrent GNs for sequences of graphs</li><li>Reinforcement learning as a method to adaptively modify graph structures during the course of computation. This would solve a personal thought experiment of training a self-aware image classifier that has the means to create new buckets. This would involve an update function that has the capacity to add/remove new buckets to the image classifier based on some spectral representation of the class output probabilities. In the formalism of GN, this is creating update/aggregation functions that can create/destroy nodes and edges.</li></ul><h1 id=quotes>Quotes<a hidden class=anchor aria-hidden=true href=#quotes>#</a></h1><blockquote><p>We suggest that a key path forward for modern AI is to commit to combinatorial generalization as a top priority.</p></blockquote><blockquote><p>When learning, we either fit new knowledge into our existing structured representations, or adjust the structure itself to better accommodate (and make use of) the new and the old.</p></blockquote><blockquote><p>Combinatorial generalization can be viewed as &ldquo;the infinite use of finite means&rdquo; - von Humboldt</p></blockquote><h1 id=notes>Notes<a hidden class=anchor aria-hidden=true href=#notes>#</a></h1><ul><li>prioritize &ldquo;combinatorial generalization&rdquo;, the ability to generate new inferences, predictions, and behaviors from know building blocks<ul><li>i.e. <em>putting the pieces together</em></li></ul></li><li>combinatorial generalization is the goal, so we should bias towards learning on structured representations and computations<ul><li>Generalizing on structured representations is easier than generalizing on unstructured representations</li></ul></li><li>The world is <em>compositional</em>, therefore our methods should be compositional<ul><li>This does not mean we mean to trade off structure for flexibility. We consider the concepts jointly (i.e. nature <em>and</em> nurture)</li></ul></li><li>Graph neural networks (GNs) exemplify this functional ambition by learning relations between &ldquo;explicitly structured data&rdquo;<ul><li>learning $\rightarrow$ flexible</li><li>structured $\rightarrow$ composing known building blocks</li></ul></li><li>GN methods contain strong <strong>relational inductive biases</strong><ol><li>Relational reasoning<ul><li><em>structure</em> is composing a set of known building blocks</li><li><em>structured representation</em> describe this composition</li><li>structured representations consist of <em>entities</em> connected by <em>relations</em> according to <em>rules</em><ul><li><em>entity</em> is an element with attributes</li><li><em>relation</em> is a property between entities</li><li><em>rule</em> is a function that maps discrete pairs of entity-relations</li></ul></li><li><em>relational reasoning</em> involves manipulating structured representations</li></ul></li><li>Inductive biases<ul><li>An <em>inductive bias</em> allows a learning algorithm to prioritize one solution over another, independent of the observed data.<ul><li>E.g. prior in Bayesian statistics, regularization terms, algorithm architecture, composition of layers in NN, etc.</li></ul></li></ul></li><li><strong>Relational Inductive Biases</strong><ul><li>&ldquo;Inductive biases which impose constraints on relationships and interactions among entities in a learning process&rdquo;</li></ul></li></ol></li><li><em>Graph Networks</em> (GN)<ul><li>$G = (u, V, E)$<ul><li><strong>u</strong>: Global attribute</li><li><strong>V</strong> is the set of nodes w/ cardinality $N^v$ where each node $v_i$ has attributes</li><li><strong>E</strong> is the set of edges w/ cardinality $N^e$ where each edge $e_v$ has attributes. $E = {(e_k,r_k,s_k)}_{k=1:N^{e}}$<ul><li>$e_k$ is the edge&rsquo;s attribute</li><li>$r_k$ is the index of the receiver node</li><li>$s_k$ is the index of the sender node</li></ul></li></ul></li><li>GN <em>block</em> := $f: G \rightarrow G$<ul><li>GN block algorithm<ol><li>Compute updated edge attributes: $e_k^{&rsquo;} = \phi^e(e_k,v_{r_k},v_{s_k},u)$</li><li>Aggregate edge attributes per node: $\bar{e}_i^{&rsquo;} = \rho^{e \rightarrow v}(E_i^{&rsquo;})$</li><li>Compute updated node attributes: $v_i^{&rsquo;} = \phi^v(\bar{e}_i^{&rsquo;}, v_i, u)$</li><li>Aggregate edge attributes globally: $\bar{e}^{&rsquo;} = \rho^{e \rightarrow u}(E^{&rsquo;})$</li><li>Aggregate node attributes globally: $\bar{v}_i^{&rsquo;} = \rho^{v \rightarrow u}(V^{&rsquo;})$</li><li>Compute updated global attribute: $u^{&rsquo;} = \phi^u(\bar{e}^{&rsquo;}, \bar{v}^{&rsquo;}, u)$</li></ol></li><li>TLDR<ul><li>Update functions: $\phi$<ul><li>SOA use NN as update function ($NN_e, NN_v, NN_u$)</li><li>Can include RNN as $\phi$, but this is not in literature yet</li></ul></li><li>Aggregation functions: $\rho$</li></ul></li></ul></li><li>Relational inductive biases in GN<ul><li>arbitrary relationships among entities. i.e. the GN architecture doesn&rsquo;t determine how entities interact.</li><li>GN are invariant to ordering on input (b/c input is graph, and graph represented as set)</li><li>update/aggregation functions are reused implying combinatorial generalization</li></ul></li></ul></li></ul><h2 id=cite>Cite<a hidden class=anchor aria-hidden=true href=#cite>#</a></h2><p>[1] Battaglia, Peter W., et al. &ldquo;Relational inductive biases, deep learning, and graph networks.&rdquo; arXiv preprint arXiv:1806.01261 (2018)</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://goodmattg.github.io>Matt's Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>