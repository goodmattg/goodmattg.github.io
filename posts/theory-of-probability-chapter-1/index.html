<!doctype html><html><head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>Theory of Probability (Chapter 1)</title>
<meta name=description content="The website of Matthew Goodman">
<meta name=author content="Matthew Goodman">
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="http://fonts.googleapis.com/css?family=Roboto" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous>
<link rel=stylesheet href=/sass/researcher.min.css>
<link rel=icon type=image/ico href=https://goodmattg.github.io/favicon.ico>
</head>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<body><div class="container mt-5">
<nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
<a class="navbar-brand mx-0 mr-sm-auto" href=https://goodmattg.github.io>
Matt Goodman
</a>
<div class="navbar-nav flex-row flex-wrap justify-content-center">
<a class="nav-item nav-link" href=/about>
About
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/posts>
Posts
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/bookshelf>
Bookshelf
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/quotes>
Quotes
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/resume>
Resume
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/contact>
Contact
</a>
</div>
</nav>
</div>
<hr>
<div id=content>
<div class=container>
<div style=text-align:center class=centered-div>
<h1 id=theory-of-probability-chapter-1>Theory of Probability (Chapter 1)</h1>
<hr>
<br>
</div>
<h3 id=retrospect-032721>Retrospect (03/27/21)</h3>
<p><em>I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don&rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.</em></p>
<h2 id=a-_gedanken_-experiment>A <em>gedanken</em> experiment</h2>
<p>Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel? Are you now master of this specific domain? Is there any feeling of accomplishment to be had? Better yet, I want to be able to review a textbook by actually saying what it did for me. Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a book gives you, especially if you haven&rsquo;t read the book for its intended purpose, to convey a body knowledge.</p>
<p>My goal is to attempt every chapter, work selected problems, and write about the experience. This is going to be a public diary of my grappling with the material. I&rsquo;m not a prodigy - so this will take a lot of effort to finish. I&rsquo;ve selected <em>&ldquo;The Theory of Probability&rdquo;</em> by Santosh S. Venkatesh. I took Professor Venkatesh&rsquo;s course &ldquo;Engineering Probability&rdquo; at Penn and did quite well. To be sure, Professor Venkatesh was one of the most engaging professors I had in college. Thank you to Oxford University Press for leaving the link to the solutions PDF in plain-text in the page source.</p>
<p>The moment I opened &ldquo;Theory&rdquo; and saw the phrase &ldquo;<em>gedanken</em> experiment&rdquo; I knew this was the book. In my limited experience, I&rsquo;ve found that when academics write textbooks, they tend to deviate from their own speaking style and revert to bland academic writing. Add to the fact that most textbooks are viewed as vehicles to deliver formulae, and it&rsquo;s clear why textbooks aren&rsquo;t beach reading material. By contrast, Professor Venkatesh doesn&rsquo;t stray a beat from his physical teaching style. I can imagine him saying everything in this book word for word, because I&rsquo;ve heard him say so many of these phrases word for word. If I could distill Venkatesh&rsquo;s style to its core, it is to ruminate on points of intellectual curiosity, not to linger on them, per se, but to give important points the cogent philosophical discussion they deserve. It&rsquo;s easy for academic text to go too far with these musings - but Venkatesh toes the line with grace.</p>
<p>Lastly I just enjoy probability. I may go to graduate school to study the subject, and see no harm in deeply understanding the material.</p>
<h2 id=chapter-11-17>Chapter 1.1-1.7</h2>
<ul>
<li>This books chapter&rsquo;s are one-indexed</li>
<li>Going through the elements of Kolmogorov representation of probability
<ul>
<li>$A=B$ iff $A \subseteq B$ and $B \subseteq A$</li>
<li>Intersection: $A \cap B$</li>
<li>Union: $A \cup B$</li>
<li>Complement: $\Omega~\backslash~A$ = $A^c$</li>
<li>$\Omega$ is <em>certainty</em></li>
<li>$\emptyset$ is <em>impossible</em></li>
</ul>
</li>
<li>For finite sets, the family of all events in the probability space, $\mathcal{F}$, is closed.
<ul>
<li>$A \in \mathcal{F} \rightarrow A^c \in \mathcal{F}$</li>
<li>$A \cup A^c = \Omega \in \mathcal{F}$</li>
<li>$(A \cup A^c)^c = \emptyset \in \mathcal{F}$</li>
</ul>
</li>
<li>Systems of sets closed under unions and complements are also called &ldquo;fields&rdquo; - author says that have more in common with algebreic systems.</li>
<li>Closure under finite unions does not guarantee closure under countably infinite unions.</li>
<li>Example:</li>
</ul>
<p>$$(a,b]^c = (0,1]~\backslash~s(a,b] = (0,a] \cup (b,1] \in R(\mathcal{J})$$</p>
<ul>
<li>This algebra is closed under finite sets of operations, yet is open under carefully chosen infinite sets of operations. The general point he works hard to convey is that the intuitive thoughts we have about probability are derived - or are intuitive only because we operate within carefully crafted $\sigma$-algebras. We have to fight to establish a notion of probability in continuous spaces. Without a scaffolding, like operating only on the Borel sets, we would reach the intractable problem that the probability of any point on the continuous real number line has probability zero. The obvious solution is to only deal with ranges of the continuous number line, which is what we do.\</li>
</ul>
<p>Probability space defined as tuple of sample space: $\Omega$, $\sigma$-algebra $\mathcal{F}$ containing all events, and probability measure $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$:</p>
<p>$$\textbf{Probability Space:}~~~~~~(\Omega, \mathcal{F}, \mathcal{P})$$</p>
<p>Venkatesh goes on a surprisingly funny tangent to show how it is that this probability space tuple is overdescribed.</p>
<p><img src=/assets/posts/ProbabilityCh1/implicitSpace.jpg alt="Probability Space">
<em>Figure 1. Samples space is impliclitly defined by event family and probability measure</em></p>
<p>My mind drifted to thinking how we develop intuition of probability spaces in everyday problem settings. In real life, we don&rsquo;t start out by knowing every possible thing that can happen, so we don&rsquo;t have $\mathcal{F}$ meaning we definitely don&rsquo;t have $\Omega$ nor $\mathcal{P}$. And when you think about it, we never actually have a complete notion of all the possibilities in anything but toy problems. The same way the Borel sets exclude all of the weird subsets on the number line that break our intuitive notion of probability, humans evolved to be very good at excluding events from our event families that have no chance of occurring. It gets at this highly complex question of how simple experiments dictate $(\Omega, \mathcal{F}, \mathcal{P})$ before we have the tuple itself. We basically end up hoisting the space up by its own bootstraps.</p>
<p>In a coin flip, $\Omega$ is ${\mathcal{H}, \mathcal{T}}$. But what if we didn&rsquo;t know what a coin was or how it worked? We would flip this mysterious coin for our first outcome $A_1$ and it would initial show $\mathcal{H}$. So $A_1 \in \omega$, the outcome space. So we have $\mathcal{H} \subseteq \mathcal{F} \subseteq \Omega$, $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$ is initially just $\mathcal{P}(\mathcal{H})=1$. Then we flip again, see $\mathcal{T}$, and suddenly this gets Bayesian. Suddenly we have an inner probability measure $P_{in}$ that&rsquo;s implicitly defined by the outer probability distribution $P_{out}$. That is, we might have actually encountered the entire sample space which would then be $\mathcal{H}, \mathcal{T}$, but we don&rsquo;t know anything about coins, so there could be more possible outcomes. Worse, we can easily see that the probability space on which $P_{out}$ is infinitely complex. A probability distribution over all the possible states that a sample space can take on isn&rsquo;t feasible. My knowledge is severely lacking here, but this feels like the idea of larger infinities at play where $\Omega_{in}$ is complexity $\aleph_1$ and $P_{out}$ is complexity $\aleph_2$.</p>
<p>$$P_{out}(\Omega_{in}~|~\omega_{in})$$</p>
<p>$$P_{out}(\mathcal{F}<em>{in}~|~\omega</em>{in})$$</p>
<p>$$P_{out}(\mathcal{P_{in}}~|~\Omega_{in}, \mathcal{F}_{in})$$</p>
<p>The key advances in AI have been making machines that are excellent at operating within well defined probability spaces and event families. We&rsquo;ll have made the next leap when machines are able to revise their outer probability distributions. For example, that doesn&rsquo;t just mean an autonomous vehicle reacting to an obstacle it has never seen before, but reacting to a completely unfamiliar situation. Autonomous vehicles are programmed to slow down when people appear in front of the vehicle. If a jaguar suddenly enters the crosswalk, the car makes the deduction that a jaguar is person-like in that it moves, and slows down accordingly. The logic may be even less complex than that. But now the autonomous car is idling at a red light and an unusually angry strongman runs up and starts trying to tip the car. What to do now? This event has no probability because it wasn&rsquo;t derivable in the context of the programmed event family. A truly intelligent vehicle would revise its notion of the probability space for driving to include other beings actively trying to impede the driving action, and react accordingly.</p>
</div>
</div><div id=footer class=mb-5>
<script src=https://utteranc.es/client.js repo=goodmattg/goodmattg.github.io issue-term=pathname label=blog theme=github-light crossorigin=anonymous async></script>
<hr>
<div class="container text-center">
<a href=https://goodmattg.github.io><small>By Matthew Goodman</small></a>
</div>
</div>
</body>
</html>