<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Theory of Probability (Chapter 1) | Matt's Log</title><meta name=keywords content><meta name=description content="Retrospect (03/27/21) I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don&rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.
A gedanken experiment Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel?"><meta name=author content="Matt Goodman"><link rel=canonical href=https://goodmattg.github.io/posts/theory-of-probability-chapter-1/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://goodmattg.github.io/magnet.ico><link rel=icon type=image/png sizes=16x16 href=https://goodmattg.github.io/magnet-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://goodmattg.github.io/magnet-32x32.ico><link rel=apple-touch-icon href=https://goodmattg.github.io/magnet-apple-touch-icon.png><link rel=mask-icon href=https://goodmattg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Theory of Probability (Chapter 1)"><meta property="og:description" content="Retrospect (03/27/21) I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don&rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.
A gedanken experiment Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel?"><meta property="og:type" content="article"><meta property="og:url" content="https://goodmattg.github.io/posts/theory-of-probability-chapter-1/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2018-02-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Theory of Probability (Chapter 1)"><meta name=twitter:description content="Retrospect (03/27/21) I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don&rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.
A gedanken experiment Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://goodmattg.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Theory of Probability (Chapter 1)","item":"https://goodmattg.github.io/posts/theory-of-probability-chapter-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Theory of Probability (Chapter 1)","name":"Theory of Probability (Chapter 1)","description":"Retrospect (03/27/21) I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don\u0026rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.\nA gedanken experiment Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel?","keywords":[],"articleBody":" Retrospect (03/27/21) I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don’t feel the pressure to take on masochistic intellectual challenges anymore.\nA gedanken experiment Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel? Are you now master of this specific domain? Is there any feeling of accomplishment to be had? Better yet, I want to be able to review a textbook by actually saying what it did for me. Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a book gives you, especially if you haven’t read the book for its intended purpose, to convey a body knowledge.\nMy goal is to attempt every chapter, work selected problems, and write about the experience. This is going to be a public diary of my grappling with the material. I’m not a prodigy - so this will take a lot of effort to finish. I’ve selected “The Theory of Probability” by Santosh S. Venkatesh. I took Professor Venkatesh’s course “Engineering Probability” at Penn and did quite well. To be sure, Professor Venkatesh was one of the most engaging professors I had in college. Thank you to Oxford University Press for leaving the link to the solutions PDF in plain-text in the page source.\nThe moment I opened “Theory” and saw the phrase “gedanken experiment” I knew this was the book. In my limited experience, I’ve found that when academics write textbooks, they tend to deviate from their own speaking style and revert to bland academic writing. Add to the fact that most textbooks are viewed as vehicles to deliver formulae, and it’s clear why textbooks aren’t beach reading material. By contrast, Professor Venkatesh doesn’t stray a beat from his physical teaching style. I can imagine him saying everything in this book word for word, because I’ve heard him say so many of these phrases word for word. If I could distill Venkatesh’s style to its core, it is to ruminate on points of intellectual curiosity, not to linger on them, per se, but to give important points the cogent philosophical discussion they deserve. It’s easy for academic text to go too far with these musings - but Venkatesh toes the line with grace.\nLastly I just enjoy probability. I may go to graduate school to study the subject, and see no harm in deeply understanding the material.\nChapter 1.1-1.7  This books chapter’s are one-indexed Going through the elements of Kolmogorov representation of probability  $A=B$ iff $A \\subseteq B$ and $B \\subseteq A$ Intersection: $A \\cap B$ Union: $A \\cup B$ Complement: $\\Omega~\\backslash~A$ = $A^c$ $\\Omega$ is certainty $\\emptyset$ is impossible   For finite sets, the family of all events in the probability space, $\\mathcal{F}$, is closed.  $A \\in \\mathcal{F} \\rightarrow A^c \\in \\mathcal{F}$ $A \\cup A^c = \\Omega \\in \\mathcal{F}$ $(A \\cup A^c)^c = \\emptyset \\in \\mathcal{F}$   Systems of sets closed under unions and complements are also called “fields” - author says that have more in common with algebreic systems. Closure under finite unions does not guarantee closure under countably infinite unions. Example:  $$(a,b]^c = (0,1]~\\backslash~s(a,b] = (0,a] \\cup (b,1] \\in R(\\mathcal{J})$$\n This algebra is closed under finite sets of operations, yet is open under carefully chosen infinite sets of operations. The general point he works hard to convey is that the intuitive thoughts we have about probability are derived - or are intuitive only because we operate within carefully crafted $\\sigma$-algebras. We have to fight to establish a notion of probability in continuous spaces. Without a scaffolding, like operating only on the Borel sets, we would reach the intractable problem that the probability of any point on the continuous real number line has probability zero. The obvious solution is to only deal with ranges of the continuous number line, which is what we do.\\  Probability space defined as tuple of sample space: $\\Omega$, $\\sigma$-algebra $\\mathcal{F}$ containing all events, and probability measure $\\mathcal{P}: \\mathcal{F} \\rightarrow \\mathbb{R}$:\n$$\\textbf{Probability Space:}~~~~~~(\\Omega, \\mathcal{F}, \\mathcal{P})$$\nVenkatesh goes on a surprisingly funny tangent to show how it is that this probability space tuple is overdescribed.\nFigure 1. Samples space is impliclitly defined by event family and probability measure\nMy mind drifted to thinking how we develop intuition of probability spaces in everyday problem settings. In real life, we don’t start out by knowing every possible thing that can happen, so we don’t have $\\mathcal{F}$ meaning we definitely don’t have $\\Omega$ nor $\\mathcal{P}$. And when you think about it, we never actually have a complete notion of all the possibilities in anything but toy problems. The same way the Borel sets exclude all of the weird subsets on the number line that break our intuitive notion of probability, humans evolved to be very good at excluding events from our event families that have no chance of occurring. It gets at this highly complex question of how simple experiments dictate $(\\Omega, \\mathcal{F}, \\mathcal{P})$ before we have the tuple itself. We basically end up hoisting the space up by its own bootstraps.\nIn a coin flip, $\\Omega$ is ${\\mathcal{H}, \\mathcal{T}}$. But what if we didn’t know what a coin was or how it worked? We would flip this mysterious coin for our first outcome $A_1$ and it would initial show $\\mathcal{H}$. So $A_1 \\in \\omega$, the outcome space. So we have $\\mathcal{H} \\subseteq \\mathcal{F} \\subseteq \\Omega$, $\\mathcal{P}: \\mathcal{F} \\rightarrow \\mathbb{R}$ is initially just $\\mathcal{P}(\\mathcal{H})=1$. Then we flip again, see $\\mathcal{T}$, and suddenly this gets Bayesian. Suddenly we have an inner probability measure $P_{in}$ that’s implicitly defined by the outer probability distribution $P_{out}$. That is, we might have actually encountered the entire sample space which would then be $\\mathcal{H}, \\mathcal{T}$, but we don’t know anything about coins, so there could be more possible outcomes. Worse, we can easily see that the probability space on which $P_{out}$ is infinitely complex. A probability distribution over all the possible states that a sample space can take on isn’t feasible. My knowledge is severely lacking here, but this feels like the idea of larger infinities at play where $\\Omega_{in}$ is complexity $\\aleph_1$ and $P_{out}$ is complexity $\\aleph_2$.\n$$P_{out}(\\Omega_{in}~|~\\omega_{in})$$\n$$P_{out}(\\mathcal{F}{in}~|~\\omega{in})$$\n$$P_{out}(\\mathcal{P_{in}}~|~\\Omega_{in}, \\mathcal{F}_{in})$$\nThe key advances in AI have been making machines that are excellent at operating within well defined probability spaces and event families. We’ll have made the next leap when machines are able to revise their outer probability distributions. For example, that doesn’t just mean an autonomous vehicle reacting to an obstacle it has never seen before, but reacting to a completely unfamiliar situation. Autonomous vehicles are programmed to slow down when people appear in front of the vehicle. If a jaguar suddenly enters the crosswalk, the car makes the deduction that a jaguar is person-like in that it moves, and slows down accordingly. The logic may be even less complex than that. But now the autonomous car is idling at a red light and an unusually angry strongman runs up and starts trying to tip the car. What to do now? This event has no probability because it wasn’t derivable in the context of the programmed event family. A truly intelligent vehicle would revise its notion of the probability space for driving to include other beings actively trying to impede the driving action, and react accordingly.\n","wordCount":"1270","inLanguage":"en","datePublished":"2018-02-17T00:00:00Z","dateModified":"2018-02-17T00:00:00Z","author":{"@type":"Person","name":"Matt Goodman"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://goodmattg.github.io/posts/theory-of-probability-chapter-1/"},"publisher":{"@type":"Organization","name":"Matt's Log","logo":{"@type":"ImageObject","url":"https://goodmattg.github.io/magnet.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://goodmattg.github.io accesskey=h title="Matt's Log (Alt + H)">Matt's Log</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://goodmattg.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://goodmattg.github.io/about title=About><span>About</span></a></li><li><a href=https://goodmattg.github.io/bookshelf title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://goodmattg.github.io/quotes title=Quotes><span>Quotes</span></a></li><li><a href=https://goodmattg.github.io/contact title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Theory of Probability (Chapter 1)</h1><div class=post-meta><span title="2018-02-17 00:00:00 +0000 UTC">February 17, 2018</span>&nbsp;·&nbsp;Matt Goodman</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#retrospect-032721 aria-label="Retrospect (03/27/21)">Retrospect (03/27/21)</a></li></ul><li><a href=#a-_gedanken_-experiment aria-label="A gedanken experiment">A <em>gedanken</em> experiment</a></li><li><a href=#chapter-11-17 aria-label="Chapter 1.1-1.7">Chapter 1.1-1.7</a></li></ul></div></details></div><div class=post-content><hr><h3 id=retrospect-032721>Retrospect (03/27/21)<a hidden class=anchor aria-hidden=true href=#retrospect-032721>#</a></h3><p><em>I remember this time in my life clearly. It was a mental high point and a physical low point. Unsurprisingly, I only made it through two chapters. Theoretical probability is hard, and I found this material impossible to work through without peer feedback. Life is meant to be lived; I don&rsquo;t feel the pressure to take on masochistic intellectual challenges anymore.</em></p><h2 id=a-_gedanken_-experiment>A <em>gedanken</em> experiment<a hidden class=anchor aria-hidden=true href=#a-_gedanken_-experiment>#</a></h2><p>Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel? Are you now master of this specific domain? Is there any feeling of accomplishment to be had? Better yet, I want to be able to review a textbook by actually saying what it did for me. Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a book gives you, especially if you haven&rsquo;t read the book for its intended purpose, to convey a body knowledge.</p><p>My goal is to attempt every chapter, work selected problems, and write about the experience. This is going to be a public diary of my grappling with the material. I&rsquo;m not a prodigy - so this will take a lot of effort to finish. I&rsquo;ve selected <em>&ldquo;The Theory of Probability&rdquo;</em> by Santosh S. Venkatesh. I took Professor Venkatesh&rsquo;s course &ldquo;Engineering Probability&rdquo; at Penn and did quite well. To be sure, Professor Venkatesh was one of the most engaging professors I had in college. Thank you to Oxford University Press for leaving the link to the solutions PDF in plain-text in the page source.</p><p>The moment I opened &ldquo;Theory&rdquo; and saw the phrase &ldquo;<em>gedanken</em> experiment&rdquo; I knew this was the book. In my limited experience, I&rsquo;ve found that when academics write textbooks, they tend to deviate from their own speaking style and revert to bland academic writing. Add to the fact that most textbooks are viewed as vehicles to deliver formulae, and it&rsquo;s clear why textbooks aren&rsquo;t beach reading material. By contrast, Professor Venkatesh doesn&rsquo;t stray a beat from his physical teaching style. I can imagine him saying everything in this book word for word, because I&rsquo;ve heard him say so many of these phrases word for word. If I could distill Venkatesh&rsquo;s style to its core, it is to ruminate on points of intellectual curiosity, not to linger on them, per se, but to give important points the cogent philosophical discussion they deserve. It&rsquo;s easy for academic text to go too far with these musings - but Venkatesh toes the line with grace.</p><p>Lastly I just enjoy probability. I may go to graduate school to study the subject, and see no harm in deeply understanding the material.</p><h2 id=chapter-11-17>Chapter 1.1-1.7<a hidden class=anchor aria-hidden=true href=#chapter-11-17>#</a></h2><ul><li>This books chapter&rsquo;s are one-indexed</li><li>Going through the elements of Kolmogorov representation of probability<ul><li>$A=B$ iff $A \subseteq B$ and $B \subseteq A$</li><li>Intersection: $A \cap B$</li><li>Union: $A \cup B$</li><li>Complement: $\Omega~\backslash~A$ = $A^c$</li><li>$\Omega$ is <em>certainty</em></li><li>$\emptyset$ is <em>impossible</em></li></ul></li><li>For finite sets, the family of all events in the probability space, $\mathcal{F}$, is closed.<ul><li>$A \in \mathcal{F} \rightarrow A^c \in \mathcal{F}$</li><li>$A \cup A^c = \Omega \in \mathcal{F}$</li><li>$(A \cup A^c)^c = \emptyset \in \mathcal{F}$</li></ul></li><li>Systems of sets closed under unions and complements are also called &ldquo;fields&rdquo; - author says that have more in common with algebreic systems.</li><li>Closure under finite unions does not guarantee closure under countably infinite unions.</li><li>Example:</li></ul><p>$$(a,b]^c = (0,1]~\backslash~s(a,b] = (0,a] \cup (b,1] \in R(\mathcal{J})$$</p><ul><li>This algebra is closed under finite sets of operations, yet is open under carefully chosen infinite sets of operations. The general point he works hard to convey is that the intuitive thoughts we have about probability are derived - or are intuitive only because we operate within carefully crafted $\sigma$-algebras. We have to fight to establish a notion of probability in continuous spaces. Without a scaffolding, like operating only on the Borel sets, we would reach the intractable problem that the probability of any point on the continuous real number line has probability zero. The obvious solution is to only deal with ranges of the continuous number line, which is what we do.\</li></ul><p>Probability space defined as tuple of sample space: $\Omega$, $\sigma$-algebra $\mathcal{F}$ containing all events, and probability measure $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$:</p><p>$$\textbf{Probability Space:}~~~~~~(\Omega, \mathcal{F}, \mathcal{P})$$</p><p>Venkatesh goes on a surprisingly funny tangent to show how it is that this probability space tuple is overdescribed.</p><p><img loading=lazy src=/assets/posts/ProbabilityCh1/implicitSpace.jpg alt="Probability Space">
<em>Figure 1. Samples space is impliclitly defined by event family and probability measure</em></p><p>My mind drifted to thinking how we develop intuition of probability spaces in everyday problem settings. In real life, we don&rsquo;t start out by knowing every possible thing that can happen, so we don&rsquo;t have $\mathcal{F}$ meaning we definitely don&rsquo;t have $\Omega$ nor $\mathcal{P}$. And when you think about it, we never actually have a complete notion of all the possibilities in anything but toy problems. The same way the Borel sets exclude all of the weird subsets on the number line that break our intuitive notion of probability, humans evolved to be very good at excluding events from our event families that have no chance of occurring. It gets at this highly complex question of how simple experiments dictate $(\Omega, \mathcal{F}, \mathcal{P})$ before we have the tuple itself. We basically end up hoisting the space up by its own bootstraps.</p><p>In a coin flip, $\Omega$ is ${\mathcal{H}, \mathcal{T}}$. But what if we didn&rsquo;t know what a coin was or how it worked? We would flip this mysterious coin for our first outcome $A_1$ and it would initial show $\mathcal{H}$. So $A_1 \in \omega$, the outcome space. So we have $\mathcal{H} \subseteq \mathcal{F} \subseteq \Omega$, $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$ is initially just $\mathcal{P}(\mathcal{H})=1$. Then we flip again, see $\mathcal{T}$, and suddenly this gets Bayesian. Suddenly we have an inner probability measure $P_{in}$ that&rsquo;s implicitly defined by the outer probability distribution $P_{out}$. That is, we might have actually encountered the entire sample space which would then be $\mathcal{H}, \mathcal{T}$, but we don&rsquo;t know anything about coins, so there could be more possible outcomes. Worse, we can easily see that the probability space on which $P_{out}$ is infinitely complex. A probability distribution over all the possible states that a sample space can take on isn&rsquo;t feasible. My knowledge is severely lacking here, but this feels like the idea of larger infinities at play where $\Omega_{in}$ is complexity $\aleph_1$ and $P_{out}$ is complexity $\aleph_2$.</p><p>$$P_{out}(\Omega_{in}~|~\omega_{in})$$</p><p>$$P_{out}(\mathcal{F}<em>{in}~|~\omega</em>{in})$$</p><p>$$P_{out}(\mathcal{P_{in}}~|~\Omega_{in}, \mathcal{F}_{in})$$</p><p>The key advances in AI have been making machines that are excellent at operating within well defined probability spaces and event families. We&rsquo;ll have made the next leap when machines are able to revise their outer probability distributions. For example, that doesn&rsquo;t just mean an autonomous vehicle reacting to an obstacle it has never seen before, but reacting to a completely unfamiliar situation. Autonomous vehicles are programmed to slow down when people appear in front of the vehicle. If a jaguar suddenly enters the crosswalk, the car makes the deduction that a jaguar is person-like in that it moves, and slows down accordingly. The logic may be even less complex than that. But now the autonomous car is idling at a red light and an unusually angry strongman runs up and starts trying to tip the car. What to do now? This event has no probability because it wasn&rsquo;t derivable in the context of the programmed event family. A truly intelligent vehicle would revise its notion of the probability space for driving to include other beings actively trying to impede the driving action, and react accordingly.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://goodmattg.github.io>Matt's Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>