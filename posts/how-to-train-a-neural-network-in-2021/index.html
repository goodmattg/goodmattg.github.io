<!doctype html><html><head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>Deep Learning in 2021</title>
<meta name=description content="The website of Matthew Goodman">
<meta name=author content="Matthew Goodman">
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="http://fonts.googleapis.com/css?family=Roboto" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous>
<link rel=stylesheet href=/sass/researcher.min.css>
<link rel=icon type=image/ico href=https://goodmattg.github.io/favicon.ico>
</head>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<body><div class="container mt-5">
<nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
<a class="navbar-brand mx-0 mr-sm-auto" href=https://goodmattg.github.io>
Matt Goodman
</a>
<div class="navbar-nav flex-row flex-wrap justify-content-center">
<a class="nav-item nav-link" href=/about>
About
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/posts>
Posts
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/bookshelf>
Bookshelf
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/quotes>
Quotes
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/contact>
Contact
</a>
</div>
</nav>
</div>
<hr>
<div id=content>
<div class=container>
<div style=text-align:center class=centered-div>
<h1 id=deep-learning-in-2021>Deep Learning in 2021</h1>
<hr>
<br>
</div>
<p>This post is going to cover the state of deep learning in 2021 with particular emphasis on AI infrastructure and deep learning tooling. If you&rsquo;re coming from a theory-based university classroom, get ready for an exhausting amount of detail. The classic pattern of &ldquo;just feed some data through the network and backpropogate&rdquo; is still the truth, but it takes ~10x more effort beyond the network to get anything useful.</p>
<h2 id=table-of-contents>Table of Contents</h2>
<div class=toc>
<nav id=TableOfContents>
<ul>
<li><a href=#table-of-contents>Table of Contents</a></li>
<li><a href=#theory>Theory</a></li>
<li><a href=#requirements>Requirements</a>
<ul>
<li><a href=#language-python>Language: Python</a></li>
</ul>
</li>
<li><a href=#cpu-training-on-machine>CPU Training on Machine</a></li>
<li><a href=#distributed-deep-learning>Distributed Deep Learning</a>
<ul>
<li><a href=#hardware>Hardware</a></li>
<li><a href=#network-switching>Network Switching</a></li>
<li><a href=#message-passing-interface-mpi>Message Passing Interface (MPI)</a></li>
<li><a href=#distributed-training-frameworks>Distributed Training Frameworks</a></li>
</ul>
</li>
<li><a href=#single-gpu-training>Single GPU Training</a></li>
<li><a href=#multi-gpu-single-node-training>Multi GPU, Single Node Training</a></li>
<li><a href=#multi-gpu-multi-node-training>Multi GPU, Multi Node Training</a></li>
<li><a href=#multi-gpu-multi-node-training-cloud>Multi GPU, Multi-Node Training Cloud</a></li>
<li><a href=#cloud-workflows-for-deep-learning>Cloud Workflows for Deep Learning</a></li>
<li><a href=#deep-learning-frameworks>Deep Learning Frameworks</a></li>
<li><a href=#higher-level-frameworks>Higher Level Frameworks</a></li>
<li><a href=#data-ingestion-patterns>Data Ingestion Patterns</a></li>
<li><a href=#frameworks>Frameworks*</a>
<ul>
<li><a href=#pytorch-ignite-with-pytorch>PyTorch Ignite with PyTorch</a></li>
<li><a href=#pytorch-lightning-with-pytorch>PyTorch Lightning with PyTorch</a></li>
<li><a href=#keras-with-tensorflow>Keras with TensorFlow</a></li>
</ul>
</li>
<li><a href=#reading-list-links>Reading List (Links)</a></li>
</ul>
</nav>
</div>
<h2 id=theory>Theory</h2>
<p>TODO</p>
<h2 id=requirements>Requirements</h2>
<h3 id=language-python>Language: Python</h3>
<p>The lingua franca of modern DL is Python. All the major research codebases are in Python, the major DL frameworks all have Python bindings, and the vast majority of tooling is written for Python users. I don&rsquo;t know of any big league research organizations that use anything other than Python.</p>
<h2 id=cpu-training-on-machine>CPU Training on Machine</h2>
<p>This is the &ldquo;Hello World&rdquo; of DL, but we&rsquo;ll explore this scenario in some detail so we can reference it in later sections. Let&rsquo;s use the <code>torchvision</code> library from the PyTorch team to train a simple classifier on MNIST.</p>
<div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4>
<table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0>
<pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">11
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">12
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">13
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">14
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">15
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">16
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">17
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">18
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">19
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">20
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">21
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">22
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">23
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">24
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">25
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">26
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">27
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">28
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">29
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">30
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">31
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">32
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">33
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">34
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">35
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">36
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">37
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">38
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">39
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">40
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">41
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">42
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">43
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">44
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">45
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">46
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">47
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">48
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">49
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">50
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">51
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">52
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">53
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">54
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">55
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">56
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">57
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">58
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">59
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">60
</span></code></pre></td>
<td style=vertical-align:top;padding:0;margin:0;border:0;width:100%>
<pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#d75f00>import</span> torch.nn <span style=color:#5f8700>as</span> nn
<span style=color:#d75f00>import</span> torch.nn.functional <span style=color:#5f8700>as</span> F
<span style=color:#d75f00>import</span> torch.optim <span style=color:#5f8700>as</span> optim
<span style=color:#d75f00>import</span> torchvision.transforms <span style=color:#5f8700>as</span> transforms

<span style=color:#d75f00>from</span> torch.utils.data <span style=color:#d75f00>import</span> DataLoader
<span style=color:#d75f00>from</span> torchvision.datasets <span style=color:#d75f00>import</span> MNIST

transform = transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize(
                                 (<span style=color:#00afaf>0.1307</span>,), (<span style=color:#00afaf>0.3081</span>,))
                             ])

<span style=color:#4e4e4e># NOTE: Dataset downloaded to local machine</span>
trainset = MNIST(root=<span style=color:#00afaf>&#39;./data&#39;</span>, train=<span style=color:#d75f00>True</span>, download=<span style=color:#d75f00>True</span>, transform=transform)
trainloader = DataLoader(trainset, batch_size=<span style=color:#00afaf>4</span>, shuffle=<span style=color:#d75f00>True</span>, num_workers=<span style=color:#00afaf>2</span>)

<span style=color:#5f8700>class</span> <span style=color:#0087ff>Net</span>(nn.Module):
    <span style=color:#5f8700>def</span> __init__(<span style=color:#0087ff>self</span>):
        <span style=color:#0087ff>super</span>(Net, <span style=color:#0087ff>self</span>).__init__()
        <span style=color:#0087ff>self</span>.conv1 = nn.Conv2d(<span style=color:#00afaf>1</span>, <span style=color:#00afaf>10</span>, kernel_size=<span style=color:#00afaf>5</span>)
        <span style=color:#0087ff>self</span>.conv2 = nn.Conv2d(<span style=color:#00afaf>10</span>, <span style=color:#00afaf>20</span>, kernel_size=<span style=color:#00afaf>5</span>)
        <span style=color:#0087ff>self</span>.conv2_drop = nn.Dropout2d()
        <span style=color:#0087ff>self</span>.fc1 = nn.Linear(<span style=color:#00afaf>320</span>, <span style=color:#00afaf>50</span>)
        <span style=color:#0087ff>self</span>.fc2 = nn.Linear(<span style=color:#00afaf>50</span>, <span style=color:#00afaf>10</span>)

    <span style=color:#5f8700>def</span> <span style=color:#0087ff>forward</span>(<span style=color:#0087ff>self</span>, x):
        x = F.relu(F.max_pool2d(<span style=color:#0087ff>self</span>.conv1(x), <span style=color:#00afaf>2</span>))
        x = F.relu(F.max_pool2d(<span style=color:#0087ff>self</span>.conv2_drop(<span style=color:#0087ff>self</span>.conv2(x)), <span style=color:#00afaf>2</span>))
        x = x.view(-<span style=color:#00afaf>1</span>, <span style=color:#00afaf>320</span>)
        x = F.relu(<span style=color:#0087ff>self</span>.fc1(x))
        x = F.dropout(x, training=<span style=color:#0087ff>self</span>.training)
        x = <span style=color:#0087ff>self</span>.fc2(x)
        <span style=color:#5f8700>return</span> F.log_softmax(x)

net = Net()

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=<span style=color:#00afaf>0.001</span>, momentum=<span style=color:#00afaf>0.9</span>)

<span style=color:#5f8700>for</span> epoch <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#00afaf>2</span>):
    running_loss = <span style=color:#00afaf>0.0</span>
    <span style=color:#5f8700>for</span> i, data <span style=color:#5f8700>in</span> <span style=color:#0087ff>enumerate</span>(trainloader, <span style=color:#00afaf>0</span>):
        <span style=color:#4e4e4e># NOTE: Data loaded from disk into RAM</span>
        inputs, labels = data

        optimizer.zero_grad()

        <span style=color:#4e4e4e># NOTE: CPU computes forward pass, backward pass, update weights</span>
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        <span style=color:#5f8700>if</span> i % <span style=color:#00afaf>2000</span> == <span style=color:#00afaf>1999</span>:
            <span style=color:#0087ff>print</span>(<span style=color:#00afaf>&#39;[</span><span style=color:#00afaf>%d</span><span style=color:#00afaf>, </span><span style=color:#00afaf>%5d</span><span style=color:#00afaf>] loss: </span><span style=color:#00afaf>%.3f</span><span style=color:#00afaf>&#39;</span> % (epoch + <span style=color:#00afaf>1</span>, i + <span style=color:#00afaf>1</span>, running_loss / <span style=color:#00afaf>2000</span>))
            running_loss = <span style=color:#00afaf>0.0</span>
</code></pre></td></tr></table>
</div>
</div><p>Several things verify that our model works correctly:</p>
<ol>
<li>There were no interpreter errors when we run the code (i.e. no crashes)</li>
<li>The loss decreases</li>
<li>The accuracy on the validation increases</li>
</ol>
<p>This example is representative of how tutorials and most universities teach deep learning, but it is not useful in practice. Let&rsquo;s dig in on several of the assumptions that make this example so elegant. First, the MNIST dataset is <strong>tiny</strong>, only 9.9 MB, and <strong>static</strong>. Why do we focus on this? Because in practice, the datasets we want to use to power insights or new features are <strong>massive</strong> and <strong>dynamic</strong> (constantly changing). It is easy in this example to download MNIST to our local device to train the model - it only takes a few seconds to make an HTTP request to the server with the tarball of the MNIST dataset and download the whole dataset. And once the dataset is in memory, the time cost of loading small batches of the dataset into our model on the CPU is <strong>effectively optimal</strong>. MNIST is just 28x28 pixel images - it takes almost no time to load a small batch of 28x28 images, and those images take up almost no RAM. You can&rsquo;t beat the speed of just loading data from disk onto CPUs without fancy I/O optimizations which we&rsquo;ll cover in a later section. If we were lazy and inefficient, we could always re-download the MNIST dataset from the server every time we trained the model and <strong>it would still work</strong>. We would only add a few seconds to each training run, and the amount of memory we consume is only 9.9 MB.</p>
<p>Also observe that all of the operations in the function above.. TODO</p>
<h2 id=distributed-deep-learning>Distributed Deep Learning</h2>
<p>Now we that we&rsquo;ve shown the example above, we can <em>throw away almost every assumption we just made</em>. In real life, DL / AI / ML is not magic. All we are doing is using backpropagation to compute gradient updates for an optimization function. The <em>magic</em> is cleverly selecting a model that maps our input data to our objective, sometimes being clever with <em>how</em> we train the model(s), and training on as much data we have available. For all of the literature on &ldquo;efficient training&rdquo; and network architectures that have stronger capacity to <em>generalize</em>, in practice we want to train the model using <strong>as much data as humanly possible</strong>. Practically any clever DL trick we can think of can be ignored if we have more training data. So if you&rsquo;re a DL practitioner in an organization, and the goal is to get a model you can actually use (in business or research), your main goal is to acquire as much data as possible. If you&rsquo;re successful and can acquire a massive dataset (>10TB), you will be able to train something useful, but your dataset <strong>does not fit in memory</strong>. Furthermore, your dataset is probably generated via an internal ETL pipeline, or by interns tasked with getting you more data, which means it <strong>changes over time</strong>.</p>
<p>It will be useful to understand the role of distributed communications in DL before digging into distributed deep training. To speed up model training on a large dataset using multiple GPUs, we turn to &ldquo;<em>data parallel training</em>&rdquo;. The plain english explanation is we can speed up model training by sending different chunks (i.e. &ldquo;<em>batches</em>") of the very large dataset to each GPU, have each GPU compute weight updates separately, and then add up those weight updates (i.e. &ldquo;<em>gradients</em>") to produce one global weight update at each time step. The step where we add up the weight updates is known as <code>all_reduce()</code>, and we&rsquo;ll cover it in more detail in the MPI section. The copy of the model on each GPU will have the same weights after each iteration of backpropagation, but we&rsquo;ve now consumed <em># gpu&rsquo;s</em> times the data in a single iteration. Throughout this process the GPU&rsquo;s need to be in constant communication, and so a distributed communications protocol is required.</p>
<p><img src=/assets/posts/DL2021/DistributedBackend.svg alt="Distributed Communication"></p>
<h3 id=hardware>Hardware</h3>
<p>The bottom layer of our distributed training stack is hardware. Our network has CPUs, GPUs, and in the future perhaps TPUs and FPGAs. For modern deep learning, the only GPUs anyone uses are sold by NVIDIA - they hold a monopoly on the market. Why? How? The short story: NVIDIA owns CUDA. CUDA makes it easy to write code that can take advantage of GPU parallelization. Deep learning, and especially computer vision based deep learning, is <strong>a lot of convolutions</strong>. Convolutions are &ldquo;shift, add, sum&rdquo; - very easy math to parallelize, <strong>extremely fast operation when parallelized</strong>. All the major numerical computation libraries built in CUDA support to use GPUs since it was first and easiest. Only NVIDIA GPUs support CUDA, so by default you can now only use NVIDIA GPUs for deep learning.</p>
<blockquote>
<p>CUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords&mldr;. These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU.</p>
</blockquote>
<p>In 2016 Google announced the Tensor Processing Unit (TPU), an application specific integrated circuit (ASIC) specifically built for deep learning. Without going into hardware detail, TPUs are significantly faster than GPUs for deep learning. GPUs are multipurpose - they are graphics processing units, literally designed to handle graphics workloads. It just so happens that GPUs are effective for deep learning, but they are not power or memory efficient. Unlike GPUs, TPUs leverage <code>systolic arrays</code> to handle large multiplications and additions with memory efficiency <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Google initially used its TPUs internally to handle its massive DL workloads, but now offers TPU enabled training and inference as a cloud offering and for sale.</p>
<h3 id=network-switching>Network Switching</h3>
<h3 id=message-passing-interface-mpi>Message Passing Interface (MPI)</h3>
<p>Message Passing Interface (MPI) surfaced out of a supercomputing computing community working group in the 1990&rsquo;s. From the MPI 4.0 specification<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>:</p>
<blockquote>
<p>MPI (Message-Passing Interface) is a message-passing library interface specification&mldr;
MPI addresses primarily the message-passing parallel programming model, in which data is moved from the address space of one process to
that of another process through cooperative operations on each process. Extensions to the
“classical” message-passing model are provided in collective operations, remote-memory
access operations, dynamic process creation, and parallel I/O</p>
</blockquote>
<p>MPI specifies <em>primitives</em> (i.e. building blocks) that can be used to compose complex distributed communications patterns. MPI specifies two main types of primitives:</p>
<ol>
<li><em>point-to-point</em> : one process communicates with another process (1:1)</li>
<li><em>collective</em> : group of processes communicates within the group (many:many)</li>
</ol>
<p>These primitives are obviously useful for distributed deep learning. Each GPU process has weight gradients we need add up? Use the <code>all_reduce()</code> primitive. Each GPU process <code>P_i</code> comes up with a tensor <code>x_i</code> that needs to be shared to all other GPU processes? Use the <code>all_gather()</code> primitive, etc. Below is a helpful graphic directly from the MPI 4.0 standard to cement the concept.</p>
<p><img src=/assets/posts/DL2021/LocalGlobalRank.svg alt="Local Global rank">
<a href=https://pytorch.org/docs/stable/notes/multiprocessing.html#multiprocessing-cuda-note>https://pytorch.org/docs/stable/notes/multiprocessing.html#multiprocessing-cuda-note</a></p>
<p><img src=/assets/posts/DL2021/MPICollectiveCommunication.png alt="Collective Communication"></p>
<h3 id=distributed-training-frameworks>Distributed Training Frameworks</h3>
<p>Horovod: <a href=https://youtu.be/SphfeTl70MI>https://youtu.be/SphfeTl70MI</a></p>
<p>Comparison of NVLink vs alternatives: <a href=https://arxiv.org/pdf/1903.04611.pdf>https://arxiv.org/pdf/1903.04611.pdf</a></p>
<h2 id=single-gpu-training>Single GPU Training</h2>
<p>This is the same scenario as &ldquo;CPU Training on Machine&rdquo;, but we now have 1 GPU in addition to our CPUs.</p>
<p><img src=/assets/posts/DL2021/SingleGPU.svg alt="Single GPU"></p>
<p><a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader</a></p>
<h2 id=multi-gpu-single-node-training>Multi GPU, Single Node Training</h2>
<p>This is the same scenario as &ldquo;Single GPU Training&rdquo;, but we now have >1 GPUs on our node. Most deep learning practitioners in academia, small industry research groups, and hobbyists operate under this scenario. Simply put, you take as many GPU&rsquo;s as you can afford and put them on a single physical machine. This avoids the need to consider inter-machine networking, and in reality, this still works for most SOTA research.</p>
<p>As seen in the figure below, the deep learning framework creates a process to manage each GPU&rsquo;s training. The GPU&rsquo;s each receive a different mini-batch of the data for the forward pass, compute gradients, and then proceed to <code>all_reduce</code>. Now we have two main ways to perform the <code>all_reduce</code>.</p>
<p>The first way is to designate one of the processes the <code>master</code> process, usually the process managing GPU 0, and have each GPU process send gradients to the <code>master</code> process. The master process accumulates all gradients and sends the <em>reduced</em> gradients back to each GPU process. Now each GPU has the same gradients to update weights. This is particularly effective for this scenario because the GPUs are all <strong>on the same machine</strong>; they are connected by a dedicated IO bus that supports extremely fast inter-GPU communication. In PyTorch this the <code>DataParallel</code>[^3] model</p>
<p>TODO: Does the second way exist??? What are the tradeoffs if so.</p>
<p><img src=/assets/posts/DL2021/MultiGPU.svg alt="Multi GPU">
image_caption</p>
<p><a href=https://pytorch.org/tutorials/intermediate/dist_tuto.html>https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>
<a href=https://pytorch.org/docs/stable/distributed.html>https://pytorch.org/docs/stable/distributed.html</a></p>
<h2 id=multi-gpu-multi-node-training>Multi GPU, Multi Node Training</h2>
<p>All processes in the process group<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> use ring communication</p>
<p><img src=/assets/posts/DL2021/MultiGPUMultiNode.svg alt="Multi GPU"></p>
<h2 id=multi-gpu-multi-node-training-cloud>Multi GPU, Multi-Node Training Cloud</h2>
<p>Truly large-scale models are trained in the multi-gpu, multi-node setting. Examples include GPT-3, DeepMind&rsquo;s DOTA model, and Google&rsquo;s DL driven recommendation algorithms. Recall that a &lsquo;node&rsquo; is typically a physical machine with 8 GPU&rsquo;s. When practitioners say a model was trained on 1024 GPU&rsquo;s, that precisely means the model was trained on 1024 / 8 = 128 physical nodes, each with 8 GPU&rsquo;s.</p>
<p>Let&rsquo;s take a moment to think of the broad challenges here before getting into specifics. We&rsquo;re now training a model across 128 separate physical machines. This introduces problems related to communication, i.e. that the physical nodes will have to communicate with each other over some network, and problems of speed. It takes time to load each batch of data from the data store to each physical machine. What if the data size is large (videos, raw audio), and what if DB reads are slow?</p>
<p>6a. Distributed Training Frameworks (<em>Training Backends</em>)
a. MPI
b. DDP (NCCL)
c. Horovod
<a href=https://github.com/horovod/horovod>https://github.com/horovod/horovod</a>
d. BytePS
e. Parameter Servers</p>
<p><a href=https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html>https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a></p>
<h2 id=cloud-workflows-for-deep-learning>Cloud Workflows for Deep Learning</h2>
<h2 id=deep-learning-frameworks>Deep Learning Frameworks</h2>
<h2 id=higher-level-frameworks>Higher Level Frameworks</h2>
<p>Keras, Ignite, Lightning</p>
<h2 id=data-ingestion-patterns>Data Ingestion Patterns</h2>
<h2 id=frameworks>Frameworks*</h2>
<p>Let&rsquo;s talk about frameworks for a second. This section has an asterisk because I want to make it clear these are opinions, not facts. Frameworks provide higher level patterns / abstractions that structure your usage of a tool and <em>in theory</em> enable you to do more complex tasks with less effort. I say in theory because at worst, the frameworks themselves have a steep learning curve and don&rsquo;t enable you to do anything the underlying tool couldn&rsquo;t do.</p>
<h3 id=pytorch-ignite-with-pytorch>PyTorch Ignite with PyTorch</h3>
<ul>
<li>Elegant <code>engine</code> construct to manage logging, hooks, training execution</li>
<li>Clean interface over PyTorch distributed module</li>
<li>Narrow set of excellent utilities for logging, checkpoint management, etc.</li>
<li>Poor support for custom code - rewriting open source code to operate with an ignite engine is difficult</li>
<li>Low-level, sits one level above PyTorch so not <em>too</em> abstract</li>
<li>Small but active developer community</li>
</ul>
<h3 id=pytorch-lightning-with-pytorch>PyTorch Lightning with PyTorch</h3>
<ul>
<li>Strong abstractions for new developers</li>
<li>Excellent support for multi GPU training</li>
<li>Integrations with upcoming deep learning backends</li>
<li>Large developer community</li>
<li>The guy who started PyTorch Lightning is shameless, he goes for personal attention at every opportunity, and they misrepresent other&rsquo;s work as the Lighning&rsquo;s work. -10.</li>
</ul>
<h3 id=keras-with-tensorflow>Keras with TensorFlow</h3>
<ul>
<li>Excellent abstractions over TensorFlow for new developers</li>
<li>Such widespread usage that Google moved to provide first-class support</li>
<li>TensorFlow adopted many core ideas</li>
<li>No longer needed to advanced TensorFlow</li>
<li>Doesn&rsquo;t play nice with the entire Google Cloud DL ecosystem</li>
</ul>
<h2 id=reading-list-links>Reading List (Links)</h2>
<ul>
<li>Andrej Karpathy&rsquo;s Recipe for Training Neural Networks (2019)</li>
<li><a href=https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/>https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/</a></li>
</ul>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p><a href=https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning>https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p><a href=https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf>https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:3 role=doc-endnote>
<p><a href=https://NEED_THE_LINK_FOR_DATA_PARALLEL.com>https://NEED_THE_LINK_FOR_DATA_PARALLEL.com</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
</div><div id=footer class=mb-5>
<script src=https://utteranc.es/client.js repo=goodmattg/goodmattg.github.io issue-term=pathname label=blog theme=github-light crossorigin=anonymous async></script>
<hr>
<div class="container text-center">
<a href=https://goodmattg.github.io><small>By Matthew Goodman</small></a>
</div>
</div>
</body>
</html>