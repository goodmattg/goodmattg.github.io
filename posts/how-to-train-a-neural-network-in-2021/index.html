<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Learning in 2021 | Matt's Log</title><meta name=keywords content><meta name=description content="Deep Learning in 2021"><meta name=author content="Matt Goodman"><link rel=canonical href=https://goodmattg.github.io/posts/how-to-train-a-neural-network-in-2021/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://goodmattg.github.io/magnet.ico><link rel=icon type=image/png sizes=16x16 href=https://goodmattg.github.io/magnet-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://goodmattg.github.io/magnet-32x32.ico><link rel=apple-touch-icon href=https://goodmattg.github.io/magnet-apple-touch-icon.png><link rel=mask-icon href=https://goodmattg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Deep Learning in 2021"><meta property="og:description" content="Deep Learning in 2021"><meta property="og:type" content="article"><meta property="og:url" content="https://goodmattg.github.io/posts/how-to-train-a-neural-network-in-2021/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-09-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-09-07T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Learning in 2021"><meta name=twitter:description content="Deep Learning in 2021"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://goodmattg.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Deep Learning in 2021","item":"https://goodmattg.github.io/posts/how-to-train-a-neural-network-in-2021/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Learning in 2021","name":"Deep Learning in 2021","description":"Deep Learning in 2021","keywords":[],"articleBody":" This post is going to cover the state of deep learning in 2021 with particular emphasis on AI infrastructure and deep learning tooling. If you’re coming from a theory-based university classroom, get ready for an exhausting amount of detail. The classic pattern of “just feed some data through the network and backpropogate” is still the truth, but it takes ~10x more effort beyond the network to get anything useful.\nTheory TODO\nRequirements Language: Python The lingua franca of modern DL is Python. All the major research codebases are in Python, the major DL frameworks all have Python bindings, and the vast majority of tooling is written for Python users. I don’t know of any big league research organizations that use anything other than Python.\nCPU Training on Machine This is the “Hello World” of DL, but we’ll explore this scenario in some detail so we can reference it in later sections. Let’s use the torchvision library from the PyTorch team to train a simple classifier on MNIST.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision.transforms as transforms from torch.utils.data import DataLoader from torchvision.datasets import MNIST transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize( (0.1307,), (0.3081,)) ]) # NOTE: Dataset downloaded to local machine trainset = MNIST(root='./data', train=True, download=True, transform=transform) trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x) net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): # NOTE: Data loaded from disk into RAM inputs, labels = data optimizer.zero_grad() # NOTE: CPU computes forward pass, backward pass, update weights outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() if i % 2000 == 1999: print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 Several things verify that our model works correctly:\nThere were no interpreter errors when we run the code (i.e. no crashes) The loss decreases The accuracy on the validation increases This example is representative of how tutorials and most universities teach deep learning, but it is not useful in practice. Let’s dig in on several of the assumptions that make this example so elegant. First, the MNIST dataset is tiny, only 9.9 MB, and static. Why do we focus on this? Because in practice, the datasets we want to use to power insights or new features are massive and dynamic (constantly changing). It is easy in this example to download MNIST to our local device to train the model - it only takes a few seconds to make an HTTP request to the server with the tarball of the MNIST dataset and download the whole dataset. And once the dataset is in memory, the time cost of loading small batches of the dataset into our model on the CPU is effectively optimal. MNIST is just 28x28 pixel images - it takes almost no time to load a small batch of 28x28 images, and those images take up almost no RAM. You can’t beat the speed of just loading data from disk onto CPUs without fancy I/O optimizations which we’ll cover in a later section. If we were lazy and inefficient, we could always re-download the MNIST dataset from the server every time we trained the model and it would still work. We would only add a few seconds to each training run, and the amount of memory we consume is only 9.9 MB.\nAlso observe that all of the operations in the function above.. TODO\nDistributed Deep Learning Now we that we’ve shown the example above, we can throw away almost every assumption we just made. In real life, DL / AI / ML is not magic. All we are doing is using backpropagation to compute gradient updates for an optimization function. The magic is cleverly selecting a model that maps our input data to our objective, sometimes being clever with how we train the model(s), and training on as much data we have available. For all of the literature on “efficient training” and network architectures that have stronger capacity to generalize, in practice we want to train the model using as much data as humanly possible. Practically any clever DL trick we can think of can be ignored if we have more training data. So if you’re a DL practitioner in an organization, and the goal is to get a model you can actually use (in business or research), your main goal is to acquire as much data as possible. If you’re successful and can acquire a massive dataset (\u003e10TB), you will be able to train something useful, but your dataset does not fit in memory. Furthermore, your dataset is probably generated via an internal ETL pipeline, or by interns tasked with getting you more data, which means it changes over time.\nIt will be useful to understand the role of distributed communications in DL before digging into distributed deep training. To speed up model training on a large dataset using multiple GPUs, we turn to “data parallel training”. The plain english explanation is we can speed up model training by sending different chunks (i.e. “batches”) of the very large dataset to each GPU, have each GPU compute weight updates separately, and then add up those weight updates (i.e. “gradients”) to produce one global weight update at each time step. The step where we add up the weight updates is known as all_reduce(), and we’ll cover it in more detail in the MPI section. The copy of the model on each GPU will have the same weights after each iteration of backpropagation, but we’ve now consumed # gpu’s times the data in a single iteration. Throughout this process the GPU’s need to be in constant communication, and so a distributed communications protocol is required.\nHardware The bottom layer of our distributed training stack is hardware. Our network has CPUs, GPUs, and in the future perhaps TPUs and FPGAs. For modern deep learning, the only GPUs anyone uses are sold by NVIDIA - they hold a monopoly on the market. Why? How? The short story: NVIDIA owns CUDA. CUDA makes it easy to write code that can take advantage of GPU parallelization. Deep learning, and especially computer vision based deep learning, is a lot of convolutions. Convolutions are “shift, add, sum” - very easy math to parallelize, extremely fast operation when parallelized. All the major numerical computation libraries built in CUDA support to use GPUs since it was first and easiest. Only NVIDIA GPUs support CUDA, so by default you can now only use NVIDIA GPUs for deep learning.\nCUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords…. These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU.\nIn 2016 Google announced the Tensor Processing Unit (TPU), an application specific integrated circuit (ASIC) specifically built for deep learning. Without going into hardware detail, TPUs are significantly faster than GPUs for deep learning. GPUs are multipurpose - they are graphics processing units, literally designed to handle graphics workloads. It just so happens that GPUs are effective for deep learning, but they are not power or memory efficient. Unlike GPUs, TPUs leverage systolic arrays to handle large multiplications and additions with memory efficiency 1. Google initially used its TPUs internally to handle its massive DL workloads, but now offers TPU enabled training and inference as a cloud offering and for sale.\nNetwork Switching Message Passing Interface (MPI) Message Passing Interface (MPI) surfaced out of a supercomputing computing community working group in the 1990’s. From the MPI 4.0 specification2:\nMPI (Message-Passing Interface) is a message-passing library interface specification… MPI addresses primarily the message-passing parallel programming model, in which data is moved from the address space of one process to that of another process through cooperative operations on each process. Extensions to the “classical” message-passing model are provided in collective operations, remote-memory access operations, dynamic process creation, and parallel I/O\nMPI specifies primitives (i.e. building blocks) that can be used to compose complex distributed communications patterns. MPI specifies two main types of primitives:\npoint-to-point : one process communicates with another process (1:1) collective : group of processes communicates within the group (many:many) These primitives are obviously useful for distributed deep learning. Each GPU process has weight gradients we need add up? Use the all_reduce() primitive. Each GPU process P_i comes up with a tensor x_i that needs to be shared to all other GPU processes? Use the all_gather() primitive, etc. Below is a helpful graphic directly from the MPI 4.0 standard to cement the concept.\nhttps://pytorch.org/docs/stable/notes/multiprocessing.html#multiprocessing-cuda-note\nDistributed Training Frameworks Horovod: https://youtu.be/SphfeTl70MI\nComparison of NVLink vs alternatives: https://arxiv.org/pdf/1903.04611.pdf\nSingle GPU Training This is the same scenario as “CPU Training on Machine”, but we now have 1 GPU in addition to our CPUs.\nhttps://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\nMulti GPU, Single Node Training This is the same scenario as “Single GPU Training”, but we now have \u003e1 GPUs on our node. Most deep learning practitioners in academia, small industry research groups, and hobbyists operate under this scenario. Simply put, you take as many GPU’s as you can afford and put them on a single physical machine. This avoids the need to consider inter-machine networking, and in reality, this still works for most SOTA research.\nAs seen in the figure below, the deep learning framework creates a process to manage each GPU’s training. The GPU’s each receive a different mini-batch of the data for the forward pass, compute gradients, and then proceed to all_reduce. Now we have two main ways to perform the all_reduce.\nThe first way is to designate one of the processes the master process, usually the process managing GPU 0, and have each GPU process send gradients to the master process. The master process accumulates all gradients and sends the reduced gradients back to each GPU process. Now each GPU has the same gradients to update weights. This is particularly effective for this scenario because the GPUs are all on the same machine; they are connected by a dedicated IO bus that supports extremely fast inter-GPU communication. In PyTorch this the DataParallel[^3] model\nTODO: Does the second way exist??? What are the tradeoffs if so.\nimage_caption\nhttps://pytorch.org/tutorials/intermediate/dist_tuto.html https://pytorch.org/docs/stable/distributed.html\nMulti GPU, Multi Node Training All processes in the process group3 use ring communication\nMulti GPU, Multi-Node Training Cloud Truly large-scale models are trained in the multi-gpu, multi-node setting. Examples include GPT-3, DeepMind’s DOTA model, and Google’s DL driven recommendation algorithms. Recall that a ’node’ is typically a physical machine with 8 GPU’s. When practitioners say a model was trained on 1024 GPU’s, that precisely means the model was trained on 1024 / 8 = 128 physical nodes, each with 8 GPU’s.\nLet’s take a moment to think of the broad challenges here before getting into specifics. We’re now training a model across 128 separate physical machines. This introduces problems related to communication, i.e. that the physical nodes will have to communicate with each other over some network, and problems of speed. It takes time to load each batch of data from the data store to each physical machine. What if the data size is large (videos, raw audio), and what if DB reads are slow?\n6a. Distributed Training Frameworks (Training Backends) a. MPI b. DDP (NCCL) c. Horovod https://github.com/horovod/horovod d. BytePS e. Parameter Servers\nhttps://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html\nCloud Workflows for Deep Learning Deep Learning Frameworks Higher Level Frameworks Keras, Ignite, Lightning\nData Ingestion Patterns Frameworks* Let’s talk about frameworks for a second. This section has an asterisk because I want to make it clear these are opinions, not facts. Frameworks provide higher level patterns / abstractions that structure your usage of a tool and in theory enable you to do more complex tasks with less effort. I say in theory because at worst, the frameworks themselves have a steep learning curve and don’t enable you to do anything the underlying tool couldn’t do.\nPyTorch Ignite with PyTorch Elegant engine construct to manage logging, hooks, training execution Clean interface over PyTorch distributed module Narrow set of excellent utilities for logging, checkpoint management, etc. Poor support for custom code - rewriting open source code to operate with an ignite engine is difficult Low-level, sits one level above PyTorch so not too abstract Small but active developer community PyTorch Lightning with PyTorch Strong abstractions for new developers Excellent support for multi GPU training Integrations with upcoming deep learning backends Large developer community The guy who started PyTorch Lightning is shameless, he goes for personal attention at every opportunity, and they misrepresent other’s work as the Lighning’s work. -10. Keras with TensorFlow Excellent abstractions over TensorFlow for new developers Such widespread usage that Google moved to provide first-class support TensorFlow adopted many core ideas No longer needed to advanced TensorFlow Doesn’t play nice with the entire Google Cloud DL ecosystem Reading List (Links) Andrej Karpathy’s Recipe for Training Neural Networks (2019) https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/ https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning ↩︎\nhttps://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf ↩︎\nhttps://NEED_THE_LINK_FOR_DATA_PARALLEL.com ↩︎\n","wordCount":"2313","inLanguage":"en","datePublished":"2021-09-07T00:00:00Z","dateModified":"2021-09-07T00:00:00Z","author":{"@type":"Person","name":"Matt Goodman"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://goodmattg.github.io/posts/how-to-train-a-neural-network-in-2021/"},"publisher":{"@type":"Organization","name":"Matt's Log","logo":{"@type":"ImageObject","url":"https://goodmattg.github.io/magnet.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://goodmattg.github.io accesskey=h title="Matt's Log (Alt + H)">Matt's Log</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://goodmattg.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://goodmattg.github.io/about title=About><span>About</span></a></li><li><a href=https://goodmattg.github.io/bookshelf title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://goodmattg.github.io/quotes title=Quotes><span>Quotes</span></a></li><li><a href=https://goodmattg.github.io/contact title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Deep Learning in 2021</h1><div class=post-description>Deep Learning in 2021</div><div class=post-meta><span title='2021-09-07 00:00:00 +0000 UTC'>September 7, 2021</span>&nbsp;·&nbsp;Matt Goodman</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#theory aria-label=Theory>Theory</a></li><li><a href=#requirements aria-label=Requirements>Requirements</a><ul><li><a href=#language-python aria-label="Language: Python">Language: Python</a></li></ul></li><li><a href=#cpu-training-on-machine aria-label="CPU Training on Machine">CPU Training on Machine</a></li><li><a href=#distributed-deep-learning aria-label="Distributed Deep Learning">Distributed Deep Learning</a><ul><li><a href=#hardware aria-label=Hardware>Hardware</a></li><li><a href=#network-switching aria-label="Network Switching">Network Switching</a></li><li><a href=#message-passing-interface-mpi aria-label="Message Passing Interface (MPI)">Message Passing Interface (MPI)</a></li><li><a href=#distributed-training-frameworks aria-label="Distributed Training Frameworks">Distributed Training Frameworks</a></li></ul></li><li><a href=#single-gpu-training aria-label="Single GPU Training">Single GPU Training</a></li><li><a href=#multi-gpu-single-node-training aria-label="Multi GPU, Single Node Training">Multi GPU, Single Node Training</a></li><li><a href=#multi-gpu-multi-node-training aria-label="Multi GPU, Multi Node Training">Multi GPU, Multi Node Training</a></li><li><a href=#multi-gpu-multi-node-training-cloud aria-label="Multi GPU, Multi-Node Training Cloud">Multi GPU, Multi-Node Training Cloud</a></li><li><a href=#cloud-workflows-for-deep-learning aria-label="Cloud Workflows for Deep Learning">Cloud Workflows for Deep Learning</a></li><li><a href=#deep-learning-frameworks aria-label="Deep Learning Frameworks">Deep Learning Frameworks</a></li><li><a href=#higher-level-frameworks aria-label="Higher Level Frameworks">Higher Level Frameworks</a></li><li><a href=#data-ingestion-patterns aria-label="Data Ingestion Patterns">Data Ingestion Patterns</a></li><li><a href=#frameworks aria-label=Frameworks*>Frameworks*</a><ul><li><a href=#pytorch-ignite-with-pytorch aria-label="PyTorch Ignite with PyTorch">PyTorch Ignite with PyTorch</a></li><li><a href=#pytorch-lightning-with-pytorch aria-label="PyTorch Lightning with PyTorch">PyTorch Lightning with PyTorch</a></li><li><a href=#keras-with-tensorflow aria-label="Keras with TensorFlow">Keras with TensorFlow</a></li></ul></li><li><a href=#reading-list-links aria-label="Reading List (Links)">Reading List (Links)</a></li></ul></div></details></div><div class=post-content><hr><p>This post is going to cover the state of deep learning in 2021 with particular emphasis on AI infrastructure and deep learning tooling. If you&rsquo;re coming from a theory-based university classroom, get ready for an exhausting amount of detail. The classic pattern of &ldquo;just feed some data through the network and backpropogate&rdquo; is still the truth, but it takes ~10x more effort beyond the network to get anything useful.</p><h2 id=theory>Theory<a hidden class=anchor aria-hidden=true href=#theory>#</a></h2><p>TODO</p><h2 id=requirements>Requirements<a hidden class=anchor aria-hidden=true href=#requirements>#</a></h2><h3 id=language-python>Language: Python<a hidden class=anchor aria-hidden=true href=#language-python>#</a></h3><p>The lingua franca of modern DL is Python. All the major research codebases are in Python, the major DL frameworks all have Python bindings, and the vast majority of tooling is written for Python users. I don&rsquo;t know of any big league research organizations that use anything other than Python.</p><h2 id=cpu-training-on-machine>CPU Training on Machine<a hidden class=anchor aria-hidden=true href=#cpu-training-on-machine>#</a></h2><p>This is the &ldquo;Hello World&rdquo; of DL, but we&rsquo;ll explore this scenario in some detail so we can reference it in later sections. Let&rsquo;s use the <code>torchvision</code> library from the PyTorch team to train a simple classifier on MNIST.</p><div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">14
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">15
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">16
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">17
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">18
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">19
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">20
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">21
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">22
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">23
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">24
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">25
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">26
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">27
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">28
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">29
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">30
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">31
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">32
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">33
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">34
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">35
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">36
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">37
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">38
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">39
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">40
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">41
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">42
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">43
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">44
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">45
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">46
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">47
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">48
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">49
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">50
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">51
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">52
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">53
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">54
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">55
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">56
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">57
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">58
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">59
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">60
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> torch.nn <span style=color:#5f8700>as</span> nn
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> torch.nn.functional <span style=color:#5f8700>as</span> F
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> torch.optim <span style=color:#5f8700>as</span> optim
</span></span><span style=display:flex><span><span style=color:#d75f00>import</span> torchvision.transforms <span style=color:#5f8700>as</span> transforms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> torch.utils.data <span style=color:#d75f00>import</span> DataLoader
</span></span><span style=display:flex><span><span style=color:#d75f00>from</span> torchvision.datasets <span style=color:#d75f00>import</span> MNIST
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transform = transforms.Compose([
</span></span><span style=display:flex><span>                               transforms.ToTensor(),
</span></span><span style=display:flex><span>                               transforms.Normalize(
</span></span><span style=display:flex><span>                                 (<span style=color:#00afaf>0.1307</span>,), (<span style=color:#00afaf>0.3081</span>,))
</span></span><span style=display:flex><span>                             ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#4e4e4e># NOTE: Dataset downloaded to local machine</span>
</span></span><span style=display:flex><span>trainset = MNIST(root=<span style=color:#00afaf>&#39;./data&#39;</span>, train=<span style=color:#d75f00>True</span>, download=<span style=color:#d75f00>True</span>, transform=transform)
</span></span><span style=display:flex><span>trainloader = DataLoader(trainset, batch_size=<span style=color:#00afaf>4</span>, shuffle=<span style=color:#d75f00>True</span>, num_workers=<span style=color:#00afaf>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>class</span> <span style=color:#0087ff>Net</span>(nn.Module):
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> __init__(<span style=color:#0087ff>self</span>):
</span></span><span style=display:flex><span>        <span style=color:#0087ff>super</span>(Net, <span style=color:#0087ff>self</span>).__init__()
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.conv1 = nn.Conv2d(<span style=color:#00afaf>1</span>, <span style=color:#00afaf>10</span>, kernel_size=<span style=color:#00afaf>5</span>)
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.conv2 = nn.Conv2d(<span style=color:#00afaf>10</span>, <span style=color:#00afaf>20</span>, kernel_size=<span style=color:#00afaf>5</span>)
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.conv2_drop = nn.Dropout2d()
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.fc1 = nn.Linear(<span style=color:#00afaf>320</span>, <span style=color:#00afaf>50</span>)
</span></span><span style=display:flex><span>        <span style=color:#0087ff>self</span>.fc2 = nn.Linear(<span style=color:#00afaf>50</span>, <span style=color:#00afaf>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>def</span> <span style=color:#0087ff>forward</span>(<span style=color:#0087ff>self</span>, x):
</span></span><span style=display:flex><span>        x = F.relu(F.max_pool2d(<span style=color:#0087ff>self</span>.conv1(x), <span style=color:#00afaf>2</span>))
</span></span><span style=display:flex><span>        x = F.relu(F.max_pool2d(<span style=color:#0087ff>self</span>.conv2_drop(<span style=color:#0087ff>self</span>.conv2(x)), <span style=color:#00afaf>2</span>))
</span></span><span style=display:flex><span>        x = x.view(-<span style=color:#00afaf>1</span>, <span style=color:#00afaf>320</span>)
</span></span><span style=display:flex><span>        x = F.relu(<span style=color:#0087ff>self</span>.fc1(x))
</span></span><span style=display:flex><span>        x = F.dropout(x, training=<span style=color:#0087ff>self</span>.training)
</span></span><span style=display:flex><span>        x = <span style=color:#0087ff>self</span>.fc2(x)
</span></span><span style=display:flex><span>        <span style=color:#5f8700>return</span> F.log_softmax(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>net = Net()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>criterion = nn.CrossEntropyLoss()
</span></span><span style=display:flex><span>optimizer = optim.SGD(net.parameters(), lr=<span style=color:#00afaf>0.001</span>, momentum=<span style=color:#00afaf>0.9</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5f8700>for</span> epoch <span style=color:#5f8700>in</span> <span style=color:#0087ff>range</span>(<span style=color:#00afaf>2</span>):
</span></span><span style=display:flex><span>    running_loss = <span style=color:#00afaf>0.0</span>
</span></span><span style=display:flex><span>    <span style=color:#5f8700>for</span> i, data <span style=color:#5f8700>in</span> <span style=color:#0087ff>enumerate</span>(trainloader, <span style=color:#00afaf>0</span>):
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># NOTE: Data loaded from disk into RAM</span>
</span></span><span style=display:flex><span>        inputs, labels = data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        optimizer.zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#4e4e4e># NOTE: CPU computes forward pass, backward pass, update weights</span>
</span></span><span style=display:flex><span>        outputs = net(inputs)
</span></span><span style=display:flex><span>        loss = criterion(outputs, labels)
</span></span><span style=display:flex><span>        loss.backward()
</span></span><span style=display:flex><span>        optimizer.step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        running_loss += loss.item()
</span></span><span style=display:flex><span>        <span style=color:#5f8700>if</span> i % <span style=color:#00afaf>2000</span> == <span style=color:#00afaf>1999</span>:
</span></span><span style=display:flex><span>            <span style=color:#0087ff>print</span>(<span style=color:#00afaf>&#39;[</span><span style=color:#00afaf>%d</span><span style=color:#00afaf>, </span><span style=color:#00afaf>%5d</span><span style=color:#00afaf>] loss: </span><span style=color:#00afaf>%.3f</span><span style=color:#00afaf>&#39;</span> % (epoch + <span style=color:#00afaf>1</span>, i + <span style=color:#00afaf>1</span>, running_loss / <span style=color:#00afaf>2000</span>))
</span></span><span style=display:flex><span>            running_loss = <span style=color:#00afaf>0.0</span>
</span></span></code></pre></td></tr></table></div></div><p>Several things verify that our model works correctly:</p><ol><li>There were no interpreter errors when we run the code (i.e. no crashes)</li><li>The loss decreases</li><li>The accuracy on the validation increases</li></ol><p>This example is representative of how tutorials and most universities teach deep learning, but it is not useful in practice. Let&rsquo;s dig in on several of the assumptions that make this example so elegant. First, the MNIST dataset is <strong>tiny</strong>, only 9.9 MB, and <strong>static</strong>. Why do we focus on this? Because in practice, the datasets we want to use to power insights or new features are <strong>massive</strong> and <strong>dynamic</strong> (constantly changing). It is easy in this example to download MNIST to our local device to train the model - it only takes a few seconds to make an HTTP request to the server with the tarball of the MNIST dataset and download the whole dataset. And once the dataset is in memory, the time cost of loading small batches of the dataset into our model on the CPU is <strong>effectively optimal</strong>. MNIST is just 28x28 pixel images - it takes almost no time to load a small batch of 28x28 images, and those images take up almost no RAM. You can&rsquo;t beat the speed of just loading data from disk onto CPUs without fancy I/O optimizations which we&rsquo;ll cover in a later section. If we were lazy and inefficient, we could always re-download the MNIST dataset from the server every time we trained the model and <strong>it would still work</strong>. We would only add a few seconds to each training run, and the amount of memory we consume is only 9.9 MB.</p><p>Also observe that all of the operations in the function above.. TODO</p><h2 id=distributed-deep-learning>Distributed Deep Learning<a hidden class=anchor aria-hidden=true href=#distributed-deep-learning>#</a></h2><p>Now we that we&rsquo;ve shown the example above, we can <em>throw away almost every assumption we just made</em>. In real life, DL / AI / ML is not magic. All we are doing is using backpropagation to compute gradient updates for an optimization function. The <em>magic</em> is cleverly selecting a model that maps our input data to our objective, sometimes being clever with <em>how</em> we train the model(s), and training on as much data we have available. For all of the literature on &ldquo;efficient training&rdquo; and network architectures that have stronger capacity to <em>generalize</em>, in practice we want to train the model using <strong>as much data as humanly possible</strong>. Practically any clever DL trick we can think of can be ignored if we have more training data. So if you&rsquo;re a DL practitioner in an organization, and the goal is to get a model you can actually use (in business or research), your main goal is to acquire as much data as possible. If you&rsquo;re successful and can acquire a massive dataset (>10TB), you will be able to train something useful, but your dataset <strong>does not fit in memory</strong>. Furthermore, your dataset is probably generated via an internal ETL pipeline, or by interns tasked with getting you more data, which means it <strong>changes over time</strong>.</p><p>It will be useful to understand the role of distributed communications in DL before digging into distributed deep training. To speed up model training on a large dataset using multiple GPUs, we turn to &ldquo;<em>data parallel training</em>&rdquo;. The plain english explanation is we can speed up model training by sending different chunks (i.e. &ldquo;<em>batches</em>&rdquo;) of the very large dataset to each GPU, have each GPU compute weight updates separately, and then add up those weight updates (i.e. &ldquo;<em>gradients</em>&rdquo;) to produce one global weight update at each time step. The step where we add up the weight updates is known as <code>all_reduce()</code>, and we&rsquo;ll cover it in more detail in the MPI section. The copy of the model on each GPU will have the same weights after each iteration of backpropagation, but we&rsquo;ve now consumed <em># gpu&rsquo;s</em> times the data in a single iteration. Throughout this process the GPU&rsquo;s need to be in constant communication, and so a distributed communications protocol is required.</p><p><img loading=lazy src=/assets/posts/DL2021/DistributedBackend.svg alt="Distributed Communication"></p><h3 id=hardware>Hardware<a hidden class=anchor aria-hidden=true href=#hardware>#</a></h3><p>The bottom layer of our distributed training stack is hardware. Our network has CPUs, GPUs, and in the future perhaps TPUs and FPGAs. For modern deep learning, the only GPUs anyone uses are sold by NVIDIA - they hold a monopoly on the market. Why? How? The short story: NVIDIA owns CUDA. CUDA makes it easy to write code that can take advantage of GPU parallelization. Deep learning, and especially computer vision based deep learning, is <strong>a lot of convolutions</strong>. Convolutions are &ldquo;shift, add, sum&rdquo; - very easy math to parallelize, <strong>extremely fast operation when parallelized</strong>. All the major numerical computation libraries built in CUDA support to use GPUs since it was first and easiest. Only NVIDIA GPUs support CUDA, so by default you can now only use NVIDIA GPUs for deep learning.</p><blockquote><p>CUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords&mldr;. These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU.</p></blockquote><p>In 2016 Google announced the Tensor Processing Unit (TPU), an application specific integrated circuit (ASIC) specifically built for deep learning. Without going into hardware detail, TPUs are significantly faster than GPUs for deep learning. GPUs are multipurpose - they are graphics processing units, literally designed to handle graphics workloads. It just so happens that GPUs are effective for deep learning, but they are not power or memory efficient. Unlike GPUs, TPUs leverage <code>systolic arrays</code> to handle large multiplications and additions with memory efficiency <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Google initially used its TPUs internally to handle its massive DL workloads, but now offers TPU enabled training and inference as a cloud offering and for sale.</p><h3 id=network-switching>Network Switching<a hidden class=anchor aria-hidden=true href=#network-switching>#</a></h3><h3 id=message-passing-interface-mpi>Message Passing Interface (MPI)<a hidden class=anchor aria-hidden=true href=#message-passing-interface-mpi>#</a></h3><p>Message Passing Interface (MPI) surfaced out of a supercomputing computing community working group in the 1990&rsquo;s. From the MPI 4.0 specification<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>:</p><blockquote><p>MPI (Message-Passing Interface) is a message-passing library interface specification&mldr;
MPI addresses primarily the message-passing parallel programming model, in which data is moved from the address space of one process to
that of another process through cooperative operations on each process. Extensions to the
“classical” message-passing model are provided in collective operations, remote-memory
access operations, dynamic process creation, and parallel I/O</p></blockquote><p>MPI specifies <em>primitives</em> (i.e. building blocks) that can be used to compose complex distributed communications patterns. MPI specifies two main types of primitives:</p><ol><li><em>point-to-point</em> : one process communicates with another process (1:1)</li><li><em>collective</em> : group of processes communicates within the group (many:many)</li></ol><p>These primitives are obviously useful for distributed deep learning. Each GPU process has weight gradients we need add up? Use the <code>all_reduce()</code> primitive. Each GPU process <code>P_i</code> comes up with a tensor <code>x_i</code> that needs to be shared to all other GPU processes? Use the <code>all_gather()</code> primitive, etc. Below is a helpful graphic directly from the MPI 4.0 standard to cement the concept.</p><p><img loading=lazy src=/assets/posts/DL2021/LocalGlobalRank.svg alt="Local Global rank">
<a href=https://pytorch.org/docs/stable/notes/multiprocessing.html#multiprocessing-cuda-note>https://pytorch.org/docs/stable/notes/multiprocessing.html#multiprocessing-cuda-note</a></p><p><img loading=lazy src=/assets/posts/DL2021/MPICollectiveCommunication.png alt="Collective Communication"></p><h3 id=distributed-training-frameworks>Distributed Training Frameworks<a hidden class=anchor aria-hidden=true href=#distributed-training-frameworks>#</a></h3><p>Horovod: <a href=https://youtu.be/SphfeTl70MI>https://youtu.be/SphfeTl70MI</a></p><p>Comparison of NVLink vs alternatives: <a href=https://arxiv.org/pdf/1903.04611.pdf>https://arxiv.org/pdf/1903.04611.pdf</a></p><h2 id=single-gpu-training>Single GPU Training<a hidden class=anchor aria-hidden=true href=#single-gpu-training>#</a></h2><p>This is the same scenario as &ldquo;CPU Training on Machine&rdquo;, but we now have 1 GPU in addition to our CPUs.</p><p><img loading=lazy src=/assets/posts/DL2021/SingleGPU.svg alt="Single GPU"></p><p><a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader</a></p><h2 id=multi-gpu-single-node-training>Multi GPU, Single Node Training<a hidden class=anchor aria-hidden=true href=#multi-gpu-single-node-training>#</a></h2><p>This is the same scenario as &ldquo;Single GPU Training&rdquo;, but we now have >1 GPUs on our node. Most deep learning practitioners in academia, small industry research groups, and hobbyists operate under this scenario. Simply put, you take as many GPU&rsquo;s as you can afford and put them on a single physical machine. This avoids the need to consider inter-machine networking, and in reality, this still works for most SOTA research.</p><p>As seen in the figure below, the deep learning framework creates a process to manage each GPU&rsquo;s training. The GPU&rsquo;s each receive a different mini-batch of the data for the forward pass, compute gradients, and then proceed to <code>all_reduce</code>. Now we have two main ways to perform the <code>all_reduce</code>.</p><p>The first way is to designate one of the processes the <code>master</code> process, usually the process managing GPU 0, and have each GPU process send gradients to the <code>master</code> process. The master process accumulates all gradients and sends the <em>reduced</em> gradients back to each GPU process. Now each GPU has the same gradients to update weights. This is particularly effective for this scenario because the GPUs are all <strong>on the same machine</strong>; they are connected by a dedicated IO bus that supports extremely fast inter-GPU communication. In PyTorch this the <code>DataParallel</code>[^3] model</p><p>TODO: Does the second way exist??? What are the tradeoffs if so.</p><p><img loading=lazy src=/assets/posts/DL2021/MultiGPU.svg alt="Multi GPU">
image_caption</p><p><a href=https://pytorch.org/tutorials/intermediate/dist_tuto.html>https://pytorch.org/tutorials/intermediate/dist_tuto.html</a>
<a href=https://pytorch.org/docs/stable/distributed.html>https://pytorch.org/docs/stable/distributed.html</a></p><h2 id=multi-gpu-multi-node-training>Multi GPU, Multi Node Training<a hidden class=anchor aria-hidden=true href=#multi-gpu-multi-node-training>#</a></h2><p>All processes in the process group<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> use ring communication</p><p><img loading=lazy src=/assets/posts/DL2021/MultiGPUMultiNode.svg alt="Multi GPU"></p><h2 id=multi-gpu-multi-node-training-cloud>Multi GPU, Multi-Node Training Cloud<a hidden class=anchor aria-hidden=true href=#multi-gpu-multi-node-training-cloud>#</a></h2><p>Truly large-scale models are trained in the multi-gpu, multi-node setting. Examples include GPT-3, DeepMind&rsquo;s DOTA model, and Google&rsquo;s DL driven recommendation algorithms. Recall that a &rsquo;node&rsquo; is typically a physical machine with 8 GPU&rsquo;s. When practitioners say a model was trained on 1024 GPU&rsquo;s, that precisely means the model was trained on 1024 / 8 = 128 physical nodes, each with 8 GPU&rsquo;s.</p><p>Let&rsquo;s take a moment to think of the broad challenges here before getting into specifics. We&rsquo;re now training a model across 128 separate physical machines. This introduces problems related to communication, i.e. that the physical nodes will have to communicate with each other over some network, and problems of speed. It takes time to load each batch of data from the data store to each physical machine. What if the data size is large (videos, raw audio), and what if DB reads are slow?</p><p>6a. Distributed Training Frameworks (<em>Training Backends</em>)
a. MPI
b. DDP (NCCL)
c. Horovod
<a href=https://github.com/horovod/horovod>https://github.com/horovod/horovod</a>
d. BytePS
e. Parameter Servers</p><p><a href=https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html>https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html</a></p><h2 id=cloud-workflows-for-deep-learning>Cloud Workflows for Deep Learning<a hidden class=anchor aria-hidden=true href=#cloud-workflows-for-deep-learning>#</a></h2><h2 id=deep-learning-frameworks>Deep Learning Frameworks<a hidden class=anchor aria-hidden=true href=#deep-learning-frameworks>#</a></h2><h2 id=higher-level-frameworks>Higher Level Frameworks<a hidden class=anchor aria-hidden=true href=#higher-level-frameworks>#</a></h2><p>Keras, Ignite, Lightning</p><h2 id=data-ingestion-patterns>Data Ingestion Patterns<a hidden class=anchor aria-hidden=true href=#data-ingestion-patterns>#</a></h2><h2 id=frameworks>Frameworks*<a hidden class=anchor aria-hidden=true href=#frameworks>#</a></h2><p>Let&rsquo;s talk about frameworks for a second. This section has an asterisk because I want to make it clear these are opinions, not facts. Frameworks provide higher level patterns / abstractions that structure your usage of a tool and <em>in theory</em> enable you to do more complex tasks with less effort. I say in theory because at worst, the frameworks themselves have a steep learning curve and don&rsquo;t enable you to do anything the underlying tool couldn&rsquo;t do.</p><h3 id=pytorch-ignite-with-pytorch>PyTorch Ignite with PyTorch<a hidden class=anchor aria-hidden=true href=#pytorch-ignite-with-pytorch>#</a></h3><ul><li>Elegant <code>engine</code> construct to manage logging, hooks, training execution</li><li>Clean interface over PyTorch distributed module</li><li>Narrow set of excellent utilities for logging, checkpoint management, etc.</li><li>Poor support for custom code - rewriting open source code to operate with an ignite engine is difficult</li><li>Low-level, sits one level above PyTorch so not <em>too</em> abstract</li><li>Small but active developer community</li></ul><h3 id=pytorch-lightning-with-pytorch>PyTorch Lightning with PyTorch<a hidden class=anchor aria-hidden=true href=#pytorch-lightning-with-pytorch>#</a></h3><ul><li>Strong abstractions for new developers</li><li>Excellent support for multi GPU training</li><li>Integrations with upcoming deep learning backends</li><li>Large developer community</li><li>The guy who started PyTorch Lightning is shameless, he goes for personal attention at every opportunity, and they misrepresent other&rsquo;s work as the Lighning&rsquo;s work. -10.</li></ul><h3 id=keras-with-tensorflow>Keras with TensorFlow<a hidden class=anchor aria-hidden=true href=#keras-with-tensorflow>#</a></h3><ul><li>Excellent abstractions over TensorFlow for new developers</li><li>Such widespread usage that Google moved to provide first-class support</li><li>TensorFlow adopted many core ideas</li><li>No longer needed to advanced TensorFlow</li><li>Doesn&rsquo;t play nice with the entire Google Cloud DL ecosystem</li></ul><h2 id=reading-list-links>Reading List (Links)<a hidden class=anchor aria-hidden=true href=#reading-list-links>#</a></h2><ul><li>Andrej Karpathy&rsquo;s Recipe for Training Neural Networks (2019)</li><li><a href=https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/>https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0/</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning>https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf>https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://NEED_THE_LINK_FOR_DATA_PARALLEL.com>https://NEED_THE_LINK_FOR_DATA_PARALLEL.com</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://goodmattg.github.io>Matt's Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement('button');e.classList.add('copy-code'),e.innerHTML='copy';function s(){e.innerHTML='copied!',setTimeout(()=>{e.innerHTML='copy'},2e3)}e.addEventListener('click',o=>{if('clipboard'in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand('copy'),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>