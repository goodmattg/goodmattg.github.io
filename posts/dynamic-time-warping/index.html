<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dynamic Time Warping for Clustering Time Series Data | Matt's Log</title><meta name=keywords content><meta name=description content="This post is as much a meditation on using Dynamic Time Warping (DTW) in production as it is a review of my work. I have so many questions about this subject. If you have any answers, I hope you will reach out."><meta name=author content="Matt Goodman"><link rel=canonical href=https://goodmattg.github.io/posts/dynamic-time-warping/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://goodmattg.github.io/magnet.ico><link rel=icon type=image/png sizes=16x16 href=https://goodmattg.github.io/magnet-16x16.ico><link rel=icon type=image/png sizes=32x32 href=https://goodmattg.github.io/magnet-32x32.ico><link rel=apple-touch-icon href=https://goodmattg.github.io/magnet-apple-touch-icon.png><link rel=mask-icon href=https://goodmattg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Dynamic Time Warping for Clustering Time Series Data"><meta property="og:description" content="This post is as much a meditation on using Dynamic Time Warping (DTW) in production as it is a review of my work. I have so many questions about this subject. If you have any answers, I hope you will reach out."><meta property="og:type" content="article"><meta property="og:url" content="https://goodmattg.github.io/posts/dynamic-time-warping/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-12-10T00:00:00+00:00"><meta property="article:modified_time" content="2017-12-10T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dynamic Time Warping for Clustering Time Series Data"><meta name=twitter:description content="This post is as much a meditation on using Dynamic Time Warping (DTW) in production as it is a review of my work. I have so many questions about this subject. If you have any answers, I hope you will reach out."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://goodmattg.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Dynamic Time Warping for Clustering Time Series Data","item":"https://goodmattg.github.io/posts/dynamic-time-warping/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dynamic Time Warping for Clustering Time Series Data","name":"Dynamic Time Warping for Clustering Time Series Data","description":"This post is as much a meditation on using Dynamic Time Warping (DTW) in production as it is a review of my work. I have so many questions about this subject. If you have any answers, I hope you will reach out.","keywords":[],"articleBody":" Motivation This was for the Wikipedia competition hosted by Kaggle competition to predict future page view counts on Wikipedia. Like so many of the solutions now the winning approach used RNNs to learn a set of hyperparameters on the entire dataset that are then fed in to a predict locally on each time series. This begs comparison to multivel hierarchical models from a Bayesian perspective, but I’ll save that for a later post. I treat these competitions as a way to explore new classes of techniques more than as competition to win. And it’s from that perspective that I settled on Dynamic Time Warping as the technique I wanted to explore. The big question I had was:\nInstead of learning hyperparameters on our entire corpus of time series, wouldn’t it be great if we could find a way to sub divide our corpus into highly similar groups, and then learn hyperparameters on those groups?\nIn the context of the [winning solution]1 this clustering layer would sit a layer above the RNN. In the language of hierarchical multilevel models, this would adding another level to the model that where we then compute posterior distributions on the hyperparameters for each group, with additional hyperparameters for the entire population. This yielded the question of how to cluster time series data. I looked to the research and settled on Dynamic Time Warping (DTW) from Keogh et. al from UC Riverside. For any pair of time series signals, we can find the energy of the signal that minimizes the distance between two input signals. DTW can easily show that time warped signals are the same. This technique has been highly effective in classifying EEG and other time-varying signals. As\nLiterature Review: DTW This review is informal but should provide sufficient grounding. Most of the important work has been done by Keogh\nRakthanmanon, T., Campana, B., Mueen, A., Batista, G., Westover, B., Zhu, Q., … Keogh, E. (n.d.). Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping.\nMueen, A., \u0026 Keogh, E. (2016). Extracting Optimal Performance from Dynamic Time Warping, 2129–2130.\nLemire, D. (2009). Faster retrieval with a two-pass dynamic-time-warping lower bound, 42, 2169–2180. http://doi.org/10.1016/j.patcog.2008.11.030\nKeogh, E. (n.d.). Clustering of Time Series Subsequences is Meaningless : Implications for Previous and Future Research.\nH. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for spoken word recognition,” IEEE Trans. Acoust. Speech, Lang. Process., vol. 26, no. 1, pp. 43–50, 1978.\nMuÌller, Meinard. Information Retrieval for Music and Motion. Springer, 2010.\nTheory: DTW For an excellent review of the computational aspects of DTW I recommend “Abdullah Mueen, Eamonn J. Keogh: Extracting Optimal Performance from Dynamic Time Warping. KDD 2016: 2129-2130”. Refer to Meinard for a solid review of DTW theory.\nDynamic Time Warping is a path-searching algorithm. DTW finds the minimum cost path between the complete matrix of pairwise distances between two time-series we will label $X$ and $Y$. Define $X := (x_1, x_2, …, x_n)$ and $Y:=(y_1, y_2,…,y_n)$. This matrix of pairwise distances is referred to as the cost matrix $C$. Define the function $c(x,y)$ as the cost or local distance function between two points $x$ and $y$. If $c(x,y)=0$, the two points are identical. Therefore, low cost implies similarity, high cost implies dissimilarity. DTW finds a path through the cost matrix of minimum total cost. Each valid path through the cost matrix is called a “warping” path. The set of all warping paths is labeled $W$.\nThis recursive function gives the minimum cost path:\n$$\\gamma(i,j)~=~d(q_i, c_j)+min{\\gamma(i-1,j-1), \\gamma(i-1, j), \\gamma(i, j-1)}$$\nFigure 1. DTW computed optimal path between sinusoid \u0026 sinusoid + sinc over $[0,6\\pi]$\nDTW Nearest-Neigbhor Clustering The Wikipedia challenge provide 145,000 time-series to predict. Ideally, I would have computed pairwise DTW distances for all 145,000 pairs, and then run agglomerative clustering with Ward linkage to generate clusters. The computation wasn’t feasible on my desktop setup, and I wasn’t willing to go the AWS route for this exercise. Instead, I decided to randomly sample 10,000 rows, compute pairwise DTWs for the row sample, run hierarchical clustering on the pairwise distance matrix, and finally assign all non-sampled rows to the cluster matching of their nearest neighbor. Admittedly this was not the ideal solution. By assigning all non-sampled rows to the cluster containing their nearest neighbor (i.e. minimizing DTW distance), I likely unbalanced the clusters. Furthermore, nearest-neighbor clustering is meaningless with high-dimensionality inputs. I considered nearest-neighbor defensible because my dimensionality was unitary. That is, I only clustered on the DTW distance and so avoided have high-dimensionality inputs. I didn’t compute any rough heuristics to quantify the distribution of non-sampled rows to clusters due to time constraints.\n“As noted by (Agrawal et al., 1993, Bradley and Fayyad, 1998), in high dimensions the very concept of nearest neighbor has little meaning, because the ratio of the distance to the nearest neighbor over the distance to the average neighbor rapidly approaches one as the dimensionality increases.” [2]\nI ending up doing a minor rewrite of Prof. Eamonn Keogh’s UCR-DTW C++ tool to do nearest-neighbor search instead of sub-sequence search. The tool now searches a list of time-series to find the one that minimizes the DTW distance instead of sub-sequence search to find the sub-sequence that minimizes distance. Because of optimizations in the UCR-DTW tool (via research by Lemire, Kim, et. al) the tool only computes the full DTW in \u003c10% of cases.\nAfter clustering, about 80,000 rows belong to one cluster - so we could conceptualize that cluster as the base data set and run our standard model. I would be curious to train the winning solution on each of my clusters to see if that improved its performance. The smaller cluster had predictable features like single large spikes, uniformity, two spikes, clear seasonality, etc. Using different linkage whole and partial linkage patterns instead of ward only increased the size of the largest cluster. This indicates that there is no signicant distinction for the majority of the time series signals in the corpus.\nCode Appendix 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %% Define original signal x = 0:pi/64:6*pi; y = 0.25*sin(x); %% Modify a section of the signal N = length(y); xind = (floor(0.40*N):floor(0.60*N)); ymod = y; ymod(xind) = sinc(x(xind)); %% Compute DTW zg = repmat(y, N, 1); zgmod = repmat(ymod', 1, N); d2 = sqrt((zg - zgmod) .^ 2); [~,ix,iy] = dtw(y, ymod); %% Plottingm [xg, yg] = meshgrid(1:N); d2(sub2ind(size(d2), iy, ix)) = 1.5*max(d2(:)); mesh(xg, yg, d2) https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795 ↩︎\n","wordCount":"1085","inLanguage":"en","datePublished":"2017-12-10T00:00:00Z","dateModified":"2017-12-10T00:00:00Z","author":{"@type":"Person","name":"Matt Goodman"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://goodmattg.github.io/posts/dynamic-time-warping/"},"publisher":{"@type":"Organization","name":"Matt's Log","logo":{"@type":"ImageObject","url":"https://goodmattg.github.io/magnet.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://goodmattg.github.io accesskey=h title="Matt's Log (Alt + H)">Matt's Log</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://goodmattg.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://goodmattg.github.io/about title=About><span>About</span></a></li><li><a href=https://goodmattg.github.io/bookshelf title=Bookshelf><span>Bookshelf</span></a></li><li><a href=https://goodmattg.github.io/quotes title=Quotes><span>Quotes</span></a></li><li><a href=https://goodmattg.github.io/contact title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Dynamic Time Warping for Clustering Time Series Data</h1><div class=post-description>This post is as much a meditation on using Dynamic Time Warping (DTW) in production as it is a review of my work. I have so many questions about this subject. If you have any answers, I hope you will reach out.</div><div class=post-meta><span title='2017-12-10 00:00:00 +0000 UTC'>December 10, 2017</span>&nbsp;·&nbsp;Matt Goodman</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#literature-review-dtw aria-label="Literature Review: DTW">Literature Review: DTW</a></li><li><a href=#theory-dtw aria-label="Theory: DTW">Theory: DTW</a></li><li><a href=#dtw-nearest-neigbhor-clustering aria-label="DTW Nearest-Neigbhor Clustering">DTW Nearest-Neigbhor Clustering</a></li><li><a href=#code-appendix aria-label="Code Appendix">Code Appendix</a></li></ul></div></details></div><div class=post-content><hr><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>This was for the Wikipedia competition hosted by Kaggle competition to predict future page view counts on Wikipedia. Like so many of the solutions now the winning approach used RNNs to learn a set of hyperparameters on the entire dataset that are then fed in to a predict locally on each time series. This begs comparison to multivel hierarchical models from a Bayesian perspective, but I&rsquo;ll save that for a later post. I treat these competitions as a way to explore new classes of techniques more than as competition to win. And it&rsquo;s from that perspective that I settled on Dynamic Time Warping as the technique I wanted to explore. The big question I had was:</p><blockquote><p>Instead of learning hyperparameters on our entire corpus of time series, wouldn&rsquo;t it be great if we could find a way to sub divide our corpus into highly <strong>similar</strong> groups, and then learn hyperparameters on those groups?</p></blockquote><p>In the context of the [winning solution]<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> this clustering layer would sit a layer above the RNN. In the language of hierarchical multilevel models, this would adding another level to the model that where we then compute posterior distributions on the hyperparameters for each group, with additional hyperparameters for the entire population. This yielded the question of how to cluster time series data.
I looked to the research and settled on Dynamic Time Warping (DTW) from Keogh et. al from UC Riverside. For any pair of time series signals, we can find the energy of the signal that minimizes the distance between two input signals. DTW can easily show that time warped signals are the same. This technique has been highly effective in classifying EEG and other time-varying signals. As</p><h2 id=literature-review-dtw>Literature Review: DTW<a hidden class=anchor aria-hidden=true href=#literature-review-dtw>#</a></h2><p>This review is informal but should provide sufficient grounding. Most of the important work has been done by Keogh</p><ol><li><p>Rakthanmanon, T., Campana, B., Mueen, A., Batista, G., Westover, B., Zhu, Q., … Keogh, E. (n.d.). Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping.</p></li><li><p>Mueen, A., & Keogh, E. (2016). Extracting Optimal Performance from Dynamic Time Warping, 2129–2130.</p></li><li><p>Lemire, D. (2009). Faster retrieval with a two-pass dynamic-time-warping lower bound, 42, 2169–2180. <a href=http://doi.org/10.1016/j.patcog.2008.11.030>http://doi.org/10.1016/j.patcog.2008.11.030</a></p></li><li><p>Keogh, E. (n.d.). Clustering of Time Series Subsequences is Meaningless : Implications for Previous and Future Research.</p></li><li><p>H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for spoken word recognition,” IEEE Trans. Acoust. Speech, Lang. Process., vol. 26, no. 1, pp. 43–50, 1978.</p></li><li><p>MuÌller, Meinard. Information Retrieval for Music and Motion. Springer, 2010.</p></li></ol><h2 id=theory-dtw>Theory: DTW<a hidden class=anchor aria-hidden=true href=#theory-dtw>#</a></h2><p>For an excellent review of the computational aspects of DTW I recommend &ldquo;Abdullah Mueen, Eamonn J. Keogh: Extracting Optimal Performance from Dynamic Time Warping. KDD 2016: 2129-2130&rdquo;. Refer to Meinard for a solid review of DTW theory.</p><p>Dynamic Time Warping is a path-searching algorithm. DTW finds the minimum cost path between the complete matrix of pairwise distances between two time-series we will label $X$ and $Y$. Define $X := (x_1, x_2, &mldr;, x_n)$ and $Y:=(y_1, y_2,&mldr;,y_n)$. This matrix of pairwise distances is referred to as the <em>cost</em> matrix $C$. Define the function $c(x,y)$ as the cost or local distance function between two points $x$ and $y$. If $c(x,y)=0$, the two points are identical. Therefore, low cost implies similarity, high cost implies dissimilarity. DTW finds a path through the cost matrix of minimum total cost. Each valid path through the cost matrix is called a &ldquo;warping&rdquo; path. The set of all warping paths is labeled $W$.</p><p>This recursive function gives the minimum cost path:</p><p>$$\gamma(i,j)~=~d(q_i, c_j)+min{\gamma(i-1,j-1), \gamma(i-1, j), \gamma(i, j-1)}$$</p><p><img loading=lazy src=/assets/posts/DTW/DTWTrace.jpg alt="DTW Trace">
<em>Figure 1. DTW computed optimal path between sinusoid & sinusoid + sinc over $[0,6\pi]$</em></p><h2 id=dtw-nearest-neigbhor-clustering>DTW Nearest-Neigbhor Clustering<a hidden class=anchor aria-hidden=true href=#dtw-nearest-neigbhor-clustering>#</a></h2><p>The Wikipedia challenge provide 145,000 time-series to predict. Ideally, I would have computed pairwise DTW distances for all 145,000 pairs, and then run agglomerative clustering with Ward linkage to generate clusters. The computation wasn&rsquo;t feasible on my desktop setup, and I wasn&rsquo;t willing to go the AWS route for this exercise. Instead, I decided to randomly sample 10,000 rows, compute pairwise DTWs for the row sample, run hierarchical clustering on the pairwise distance matrix, and finally assign all non-sampled rows to the cluster matching of their nearest neighbor. Admittedly this was not the ideal solution. By assigning all non-sampled rows to the cluster containing their nearest neighbor (i.e. minimizing DTW distance), I likely unbalanced the clusters. Furthermore, nearest-neighbor clustering is meaningless with high-dimensionality inputs. I considered nearest-neighbor defensible because my dimensionality was unitary. That is, I only clustered on the DTW distance and so avoided have high-dimensionality inputs. I didn&rsquo;t compute any rough heuristics to quantify the distribution of non-sampled rows to clusters due to time constraints.</p><blockquote><p>&ldquo;As noted by (Agrawal et al., 1993, Bradley and Fayyad, 1998), in high dimensions the very concept of nearest neighbor has little meaning, because the ratio of the distance to the nearest neighbor over the distance to the average neighbor rapidly approaches one as the dimensionality increases.&rdquo; [2]</p></blockquote><p>I ending up doing a minor rewrite of Prof. Eamonn Keogh&rsquo;s UCR-DTW C++ tool to do nearest-neighbor search instead of sub-sequence search. The tool now searches a list of time-series to find the one that minimizes the DTW distance instead of sub-sequence search to find the sub-sequence that minimizes distance. Because of optimizations in the UCR-DTW tool (via research by Lemire, Kim, et. al) the tool only computes the full DTW in &lt;10% of cases.</p><p>After clustering, about 80,000 rows belong to one cluster - so we could conceptualize that cluster as the base data set and run our standard model. I would be curious to train the winning solution on each of my clusters to see if that improved its performance. The smaller cluster had predictable features like single large spikes, uniformity, two spikes, clear seasonality, etc. Using different linkage whole and partial linkage patterns instead of ward only increased the size of the largest cluster. This indicates that there is no signicant distinction for the majority of the time series signals in the corpus.</p><h2 id=code-appendix>Code Appendix<a hidden class=anchor aria-hidden=true href=#code-appendix>#</a></h2><div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">11
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">12
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">13
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">14
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">15
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">16
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">17
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">18
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">19
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">20
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#454545">21
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e>%% Define original signal</span>
</span></span><span style=display:flex><span>    x = <span style=color:#00afaf>0</span>:<span style=color:#0087ff>pi</span>/<span style=color:#00afaf>64</span>:<span style=color:#00afaf>6</span>*<span style=color:#0087ff>pi</span>;
</span></span><span style=display:flex><span>    y = <span style=color:#00afaf>0.25</span>*<span style=color:#0087ff>sin</span>(x);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e>%% Modify a section of the signal</span>
</span></span><span style=display:flex><span>    N = <span style=color:#0087ff>length</span>(y);
</span></span><span style=display:flex><span>    xind = (<span style=color:#0087ff>floor</span>(<span style=color:#00afaf>0.40</span>*N):<span style=color:#0087ff>floor</span>(<span style=color:#00afaf>0.60</span>*N));
</span></span><span style=display:flex><span>    ymod = y;
</span></span><span style=display:flex><span>    ymod(xind) = sinc(x(xind));
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e>%% Compute DTW</span>
</span></span><span style=display:flex><span>    zg = <span style=color:#0087ff>repmat</span>(y, N, <span style=color:#00afaf>1</span>);
</span></span><span style=display:flex><span>    zgmod = <span style=color:#0087ff>repmat</span>(ymod&#39;, <span style=color:#00afaf>1</span>, N);
</span></span><span style=display:flex><span>    d2 = <span style=color:#0087ff>sqrt</span>((zg - zgmod) .^ <span style=color:#00afaf>2</span>);
</span></span><span style=display:flex><span>    [~,ix,iy] = dtw(y, ymod);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#4e4e4e>%% Plottingm</span>
</span></span><span style=display:flex><span>    [xg, yg] = <span style=color:#0087ff>meshgrid</span>(<span style=color:#00afaf>1</span>:N);
</span></span><span style=display:flex><span>    d2(<span style=color:#0087ff>sub2ind</span>(<span style=color:#0087ff>size</span>(d2), iy, ix)) = <span style=color:#00afaf>1.5</span>*max(d2(:));
</span></span><span style=display:flex><span>    mesh(xg, yg, d2)
</span></span></code></pre></td></tr></table></div></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795>https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://goodmattg.github.io>Matt's Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>