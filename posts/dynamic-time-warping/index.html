<!doctype html><html><head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>Dynamic Time Warping for Clustering Time Series Data</title>
<meta name=description content="The website of Matthew Goodman">
<meta name=author content="Matthew Goodman">
<link rel=preconnect href=https://fonts.gstatic.com>
<link href="https://fonts.googleapis.com/css?family=Roboto" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous>
<link rel=stylesheet href=/sass/researcher.min.css>
<link rel=icon type=image/ico href=https://goodmattg.github.io/favicon.ico>
</head>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script>
<body><div class="container mt-5">
<nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
<a class="navbar-brand mx-0 mr-sm-auto" href=https://goodmattg.github.io>
Matt Goodman
</a>
<div class="navbar-nav flex-row flex-wrap justify-content-center">
<a class="nav-item nav-link" href=/about>
About
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/posts>
Posts
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/bookshelf>
Bookshelf
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/quotes>
Quotes
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/contact>
Contact
</a>
<span class="nav-item navbar-text mx-1">|</span>
<a class="nav-item nav-link" href=/sync-motion>
SyncMotion
</a>
</div>
</nav>
</div>
<hr>
<div id=content>
<div class=container>
<div style=text-align:center class=centered-div>
<h1 id=dynamic-time-warping-for-clustering-time-series-data>Dynamic Time Warping for Clustering Time Series Data</h1>
<hr>
<br>
</div>
<h2 id=motivation>Motivation</h2>
<p>This was for the Wikipedia competition hosted by Kaggle competition to predict future page view counts on Wikipedia. Like so many of the solutions now the winning approach used RNNs to learn a set of hyperparameters on the entire dataset that are then fed in to a predict locally on each time series. This begs comparison to multivel hierarchical models from a Bayesian perspective, but I&rsquo;ll save that for a later post. I treat these competitions as a way to explore new classes of techniques more than as competition to win. And it&rsquo;s from that perspective that I settled on Dynamic Time Warping as the technique I wanted to explore. The big question I had was:</p>
<blockquote>
<p>Instead of learning hyperparameters on our entire corpus of time series, wouldn&rsquo;t it be great if we could find a way to sub divide our corpus into highly <strong>similar</strong> groups, and then learn hyperparameters on those groups?</p>
</blockquote>
<p>In the context of the [winning solution]<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> this clustering layer would sit a layer above the RNN. In the language of hierarchical multilevel models, this would adding another level to the model that where we then compute posterior distributions on the hyperparameters for each group, with additional hyperparameters for the entire population. This yielded the question of how to cluster time series data.
I looked to the research and settled on Dynamic Time Warping (DTW) from Keogh et. al from UC Riverside. For any pair of time series signals, we can find the energy of the signal that minimizes the distance between two input signals. DTW can easily show that time warped signals are the same. This technique has been highly effective in classifying EEG and other time-varying signals. As</p>
<h2 id=literature-review-dtw>Literature Review: DTW</h2>
<p>This review is informal but should provide sufficient grounding. Most of the important work has been done by Keogh</p>
<ol>
<li>
<p>Rakthanmanon, T., Campana, B., Mueen, A., Batista, G., Westover, B., Zhu, Q., … Keogh, E. (n.d.). Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping.</p>
</li>
<li>
<p>Mueen, A., & Keogh, E. (2016). Extracting Optimal Performance from Dynamic Time Warping, 2129–2130.</p>
</li>
<li>
<p>Lemire, D. (2009). Faster retrieval with a two-pass dynamic-time-warping lower bound, 42, 2169–2180. <a href=http://doi.org/10.1016/j.patcog.2008.11.030>http://doi.org/10.1016/j.patcog.2008.11.030</a></p>
</li>
<li>
<p>Keogh, E. (n.d.). Clustering of Time Series Subsequences is Meaningless : Implications for Previous and Future Research.</p>
</li>
<li>
<p>H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for spoken word recognition,” IEEE Trans. Acoust. Speech, Lang. Process., vol. 26, no. 1, pp. 43–50, 1978.</p>
</li>
<li>
<p>MuÌller, Meinard. Information Retrieval for Music and Motion. Springer, 2010.</p>
</li>
</ol>
<h2 id=theory-dtw>Theory: DTW</h2>
<p>For an excellent review of the computational aspects of DTW I recommend &ldquo;Abdullah Mueen, Eamonn J. Keogh: Extracting Optimal Performance from Dynamic Time Warping. KDD 2016: 2129-2130&rdquo;. Refer to Meinard for a solid review of DTW theory.</p>
<p>Dynamic Time Warping is a path-searching algorithm. DTW finds the minimum cost path between the complete matrix of pairwise distances between two time-series we will label $X$ and $Y$. Define $X := (x_1, x_2, &mldr;, x_n)$ and $Y:=(y_1, y_2,&mldr;,y_n)$. This matrix of pairwise distances is referred to as the <em>cost</em> matrix $C$. Define the function $c(x,y)$ as the cost or local distance function between two points $x$ and $y$. If $c(x,y)=0$, the two points are identical. Therefore, low cost implies similarity, high cost implies dissimilarity. DTW finds a path through the cost matrix of minimum total cost. Each valid path through the cost matrix is called a &ldquo;warping&rdquo; path. The set of all warping paths is labeled $W$.</p>
<p>This recursive function gives the minimum cost path:</p>
<p>$$\gamma(i,j)~=~d(q_i, c_j)+min{\gamma(i-1,j-1), \gamma(i-1, j), \gamma(i, j-1)}$$</p>
<p><img src=/assets/posts/DTW/DTWTrace.jpg alt="DTW Trace">
<em>Figure 1. DTW computed optimal path between sinusoid & sinusoid + sinc over $[0,6\pi]$</em></p>
<h2 id=dtw-nearest-neigbhor-clustering>DTW Nearest-Neigbhor Clustering</h2>
<p>The Wikipedia challenge provide 145,000 time-series to predict. Ideally, I would have computed pairwise DTW distances for all 145,000 pairs, and then run agglomerative clustering with Ward linkage to generate clusters. The computation wasn&rsquo;t feasible on my desktop setup, and I wasn&rsquo;t willing to go the AWS route for this exercise. Instead, I decided to randomly sample 10,000 rows, compute pairwise DTWs for the row sample, run hierarchical clustering on the pairwise distance matrix, and finally assign all non-sampled rows to the cluster matching of their nearest neighbor. Admittedly this was not the ideal solution. By assigning all non-sampled rows to the cluster containing their nearest neighbor (i.e. minimizing DTW distance), I likely unbalanced the clusters. Furthermore, nearest-neighbor clustering is meaningless with high-dimensionality inputs. I considered nearest-neighbor defensible because my dimensionality was unitary. That is, I only clustered on the DTW distance and so avoided have high-dimensionality inputs. I didn&rsquo;t compute any rough heuristics to quantify the distribution of non-sampled rows to clusters due to time constraints.</p>
<blockquote>
<p>&ldquo;As noted by (Agrawal et al., 1993, Bradley and Fayyad, 1998), in high dimensions the very concept of nearest neighbor has little meaning, because the ratio of the distance to the nearest neighbor over the distance to the average neighbor rapidly approaches one as the dimensionality increases.&rdquo; [2]</p>
</blockquote>
<p>I ending up doing a minor rewrite of Prof. Eamonn Keogh&rsquo;s UCR-DTW C++ tool to do nearest-neighbor search instead of sub-sequence search. The tool now searches a list of time-series to find the one that minimizes the DTW distance instead of sub-sequence search to find the sub-sequence that minimizes distance. Because of optimizations in the UCR-DTW tool (via research by Lemire, Kim, et. al) the tool only computes the full DTW in &lt;10% of cases.</p>
<p>After clustering, about 80,000 rows belong to one cluster - so we could conceptualize that cluster as the base data set and run our standard model. I would be curious to train the winning solution on each of my clusters to see if that improved its performance. The smaller cluster had predictable features like single large spikes, uniformity, two spikes, clear seasonality, etc. Using different linkage whole and partial linkage patterns instead of ward only increased the size of the largest cluster. This indicates that there is no signicant distinction for the majority of the time series signals in the corpus.</p>
<h2 id=code-appendix>Code Appendix</h2>
<div class=highlight><div style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4>
<table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0>
<pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">11
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">12
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">13
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">14
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">15
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">16
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">17
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">18
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">19
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">20
</span><span style="margin-right:.4em;padding:0 .4em;color:#454545">21
</span></code></pre></td>
<td style=vertical-align:top;padding:0;margin:0;border:0;width:100%>
<pre tabindex=0 style=color:#8a8a8a;background-color:#1c1c1c;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab>
    <span style=color:#4e4e4e>%% Define original signal</span>
    x = <span style=color:#00afaf>0</span>:<span style=color:#0087ff>pi</span>/<span style=color:#00afaf>64</span>:<span style=color:#00afaf>6</span>*<span style=color:#0087ff>pi</span>;
    y = <span style=color:#00afaf>0.25</span>*<span style=color:#0087ff>sin</span>(x);

    <span style=color:#4e4e4e>%% Modify a section of the signal</span>
    N = <span style=color:#0087ff>length</span>(y);
    xind = (<span style=color:#0087ff>floor</span>(<span style=color:#00afaf>0.40</span>*N):<span style=color:#0087ff>floor</span>(<span style=color:#00afaf>0.60</span>*N));
    ymod = y;
    ymod(xind) = sinc(x(xind));

    <span style=color:#4e4e4e>%% Compute DTW</span>
    zg = <span style=color:#0087ff>repmat</span>(y, N, <span style=color:#00afaf>1</span>);
    zgmod = <span style=color:#0087ff>repmat</span>(ymod&#39;, <span style=color:#00afaf>1</span>, N);
    d2 = <span style=color:#0087ff>sqrt</span>((zg - zgmod) .^ <span style=color:#00afaf>2</span>);
    [~,ix,iy] = dtw(y, ymod);

    <span style=color:#4e4e4e>%% Plottingm</span>
    [xg, yg] = <span style=color:#0087ff>meshgrid</span>(<span style=color:#00afaf>1</span>:N);
    d2(<span style=color:#0087ff>sub2ind</span>(<span style=color:#0087ff>size</span>(d2), iy, ix)) = <span style=color:#00afaf>1.5</span>*max(d2(:));
    mesh(xg, yg, d2)
</code></pre></td></tr></table>
</div>
</div><section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p><a href=https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795>https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
</div><div id=footer class=mb-5>
<script src=https://utteranc.es/client.js repo=goodmattg/goodmattg.github.io issue-term=pathname label=blog theme=github-light crossorigin=anonymous async></script>
<hr>
<div class="container text-center">
<a href=https://goodmattg.github.io><small>By Matthew Goodman</small></a>
</div>
</div>
</body>
</html>