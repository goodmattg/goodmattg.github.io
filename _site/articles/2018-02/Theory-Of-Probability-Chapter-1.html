<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a b...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Theory of Probability (Chapter 1) | Matt Goodman&#39;s Blog">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Theory of Probability (Chapter 1) | Matt Goodman&#39;s Blog">
  <meta name="twitter:description" content="Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a b...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1">
  <meta property="og:title" content="Theory of Probability (Chapter 1) | Matt Goodman&#39;s Blog">
  <meta property="og:description" content="Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a b...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Theory of Probability (Chapter 1) | Matt Goodman's Blog</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1">
  <link rel="alternate" type="application/rss+xml" title="Matt Goodman&#39;s Blog" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/" class="author_name">Matt Goodman's Blog</a>
  <span class="author_job">It's All Good, Man</span>
  <span class="author_bio mbm">A selection of my research and thoughts</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="http://localhost:4000/">home</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/archive/">Archive</a>
      </li>
          
      <li class="nav-item">
        <a href="http://localhost:4000/categories/">Categories</a>
      </li>
            
      <li class="nav-item">
        <a href="http://localhost:4000/resume/">Resume</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/tags/">Tags</a>
      </li>
         
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('mattgoodman13 (.a.t.) gmail (.d.o.t.) com', 'Hello from website');</script>
      </li>
    
    
    
    
    <li><a href="http://linkedin.com/in/matthew-goodman-89b76989" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    
    <li><a href="http://github.com/goodmattg" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <a class="btn" href= "http://localhost:4000/" >
  Home
</a>



<div id="post">
  <header class="post-header">
    <h1 title="Theory of Probability (Chapter 1)">Theory of Probability (Chapter 1)</h1>
    <span class="post-meta">
      <span class="post-date">
        17 FEB 2018
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    6 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h2 id="a-gedanken-experiment">A <em>gedanken</em> experiment</h2>

<p>Do you gain some sort of magic insight if you work through an entire textbook, or is there no light at the end of the tunnel? Are you now master of this specific domain? Is there any feeling of accomplishment to be had? Better yet, I want to be able to review a textbook by actually saying what it did for me. Reviewing a cleaning product is easy - you use the product and it does or does not work. Reviewing a textbook is harder - how do you quantify the benefit a book gives you, especially if you haven’t read the book for its intended purpose, to convey a body knowledge.</p>

<p>My goal is to attempt every chapter, work selected problems, and write about the experience. This is going to be a public diary of my grappling with the material. I’m not a prodigy - so this will take a lot of effort to finish. I’ve selected <em>“The Theory of Probability”</em> by Santosh S. Venkatesh. I took Professor Venkatesh’s course “Engineering Probability” at Penn and did quite well. To be sure, Professor Venkatesh was one of the most engaging professors I had in college. Thank you to Oxford University Press for leaving the link to the solutions PDF in plain-text in the page source.</p>

<p>The moment I opened “Theory” and saw the phrase “<em>gedanken</em> experiment” I knew this was the book. In my limited experience, I’ve found that when academics write textbooks, they tend to deviate from their own speaking style and revert to bland academic writing. Add to the fact that most textbooks are viewed as vehicles to deliver formulae, and it’s clear why textbooks aren’t beach reading material. By contrast, Professor Venkatesh doesn’t stray a beat from his physical teaching style. I can imagine him saying everything in this book word for word, because I’ve heard him say so many of these phrases word for word. If I could distill Venkatesh’s style to its core, it is to ruminate on points of intellectual curiosity, not to linger on them, per se, but to give important points the cogent philosophical discussion they deserve. It’s easy for academic text to go too far with these musings - but Venkatesh toes the line with grace.</p>

<p>Lastly I just enjoy probability. I may go to graduate school to study the subject, and see no harm in deeply understanding the material.</p>

<h2 id="chapter-11-17">Chapter 1.1-1.7</h2>

<ul>
  <li>This books chapter’s are one-indexed</li>
  <li>Going through the elements of Kolmogorov representation of probability
    <ul>
      <li>$A=B$ iff $A \subseteq B$ and $B \subseteq A$</li>
      <li>Intersection: $A \cap B$</li>
      <li>Union: $A \cup B$</li>
      <li>Complement: $\Omega~\backslash~A$ = $A^c$</li>
      <li>$\Omega$ is <em>certainty</em></li>
      <li>$\emptyset$ is <em>impossible</em></li>
    </ul>
  </li>
  <li>For finite sets, the family of all events in the probability space, $\mathcal{F}$, is closed.
    <ul>
      <li>$A \in \mathcal{F} \rightarrow A^c \in \mathcal{F}$</li>
      <li>$A \cup A^c = \Omega \in \mathcal{F}$</li>
      <li>$(A \cup A^c)^c = \emptyset \in \mathcal{F}$</li>
    </ul>
  </li>
  <li>Systems of sets closed under unions and complements are also called “fields” - author says that have more in common with algebreic systems.</li>
  <li>Closure under finite unions does not guarantee closure under countably infinite unions.</li>
  <li>Example:</li>
</ul>

<script type="math/tex; mode=display">(a,b]^c = (0,1]~\backslash~s(a,b] = (0,a] \cup (b,1] \in R(\mathcal{J})</script>

<ul>
  <li>This algebra is closed under finite sets of operations, yet is open under carefully chosen infinite sets of operations. The general point he works hard to convey is that the intuitive thoughts we have about probability are derived - or are intuitive only because we operate within carefully crafted $\sigma$-algebras. We have to fight to establish a notion of probability in continuous spaces. Without a scaffolding, like operating only on the Borel sets, we would reach the intractable problem that the probability of any point on the continuous real number line has probability zero. The obvious solution is to only deal with ranges of the continuous number line, which is what we do.\</li>
</ul>

<p>Probability space defined as tuple of sample space: $\Omega$, $\sigma$-algebra $\mathcal{F}$ containing all events, and probability measure $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$:</p>

<script type="math/tex; mode=display">\textbf{Probability Space:}~~~~~~(\Omega, \mathcal{F}, \mathcal{P})</script>

<p>Venkatesh goes on a surprisingly funny tangent to show how it is that this probability space tuple is overdescribed.</p>

<p><img src="http://localhost:4000/assets/posts/ProbabilityCh1/implicitSpace.jpg" alt="DTW Trace" class="center-image" />
<em>Figure 1. Samples space is impliclitly defined by event family and probability measure</em></p>

<p>My mind drifted to thinking how we develop intuition of probability spaces in everyday problem settings. In real life, we don’t start out by knowing every possible thing that can happen, so we don’t have $\mathcal{F}$ meaning we definitely don’t have $\Omega$ nor $\mathcal{P}$. And when you think about it, we never actually have a complete notion of all the possibilities in anything but toy problems. The same way the Borel sets exclude all of the weird subsets on the number line that break our intuitive notion of probability, humans evolved to be very good at excluding events from our event families that have no chance of occurring. It gets at this highly complex question of how simple experiments dictate $(\Omega, \mathcal{F}, \mathcal{P})$ before we have the tuple itself. We basically end up hoisting the space up by its own bootstraps.</p>

<p>In a coin flip, $\Omega$ is ${\mathcal{H}, \mathcal{T}}$. But what if we didn’t know what a coin was or how it worked? We would flip this mysterious coin for our first outcome $A_1$ and it would initial show $\mathcal{H}$. So $A_1 \in \omega$, the outcome space. So we have $\mathcal{H} \subseteq \mathcal{F} \subseteq \Omega$, $\mathcal{P}: \mathcal{F} \rightarrow \mathbb{R}$ is initially just $\mathcal{P}(\mathcal{H})=1$. Then we flip again, see $\mathcal{T}$, and suddenly this gets Bayesian. Suddenly we have an inner probability measure $P_{in}$ that’s implicitly defined by the outer probability distribution $P_{out}$. That is, we might have actually encountered the entire sample space which would then be $\mathcal{H}, \mathcal{T}$, but we don’t know anything about coins, so there could be more possible outcomes. Worse, we can easily see that the probability space on which $P_{out}$ is infinitely complex. A probability distribution over all the possible states that a sample space can take on isn’t feasible. My knowledge is severely lacking here, but this feels like the idea of larger infinities at play where $\Omega_{in}$ is complexity $\aleph_1$ and $P_{out}$ is complexity $\aleph_2$.</p>

<script type="math/tex; mode=display">P_{out}(\Omega_{in}~|~\omega_{in})</script>

<script type="math/tex; mode=display">P_{out}(\mathcal{F}_{in}~|~\omega_{in})</script>

<script type="math/tex; mode=display">P_{out}(\mathcal{P_{in}}~|~\Omega_{in}, \mathcal{F}_{in})</script>

<p>The key advances in AI have been making machines that are excellent at operating within well defined probability spaces and event families. We’ll have made the next leap when machines are able to revise their outer probability distributions. For example, that doesn’t just mean an autonomous vehicle reacting to an obstacle it has never seen before, but reacting to a completely unfamiliar situation. Autonomous vehicles are programmed to slow down when people appear in front of the vehicle. If a jaguar suddenly enters the crosswalk, the car makes the deduction that a jaguar is person-like in that it moves, and slows down accordingly. The logic may be even less complex than that. But now the autonomous car is idling at a red light and an unusually angry strongman runs up and starts trying to tip the car. What to do now? This event has no probability because it wasn’t derivable in the context of the programmed event family. A truly intelligent vehicle would revise its notion of the probability space for driving to include other beings actively trying to impede the driving action, and react accordingly.</p>

  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=http://localhost:4000/articles/2018-02/Theory-Of-Probability-Chapter-1" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->


<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'https-goodmattg-github-io';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



        <footer>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
  &copy; 2019 Matt Goodman's Blog. Powered by <a href="http://jekyllrb.com/">Jekyll</a>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>


</body>
</html>
